{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Code as Policy for Metaworld\n",
    "\n",
    "\n",
    "This notebook shows a basic example of using the optimizer to follow language feedback online during interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "seed = 0\n",
    "horizon = 30\n",
    "env_name = \"llf-metaworld-pick-place-v2\"\n",
    "stepsize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llfbench\n",
    "import autogen.trace as trace\n",
    "from autogen.trace.optimizers import FunctionOptimizer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TracedEnv:\n",
    "\n",
    "    def __init__(self, env_name, seed=0):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.env = llfbench.make(env_name)\n",
    "        self.env.reset(seed=seed)\n",
    "        self.env.action_space.seed(seed)\n",
    "\n",
    "    @trace.trace_op()\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial observation and info.\n",
    "        \"\"\"\n",
    "        return self.env.reset()  # obs, info\n",
    "\n",
    "    @trace.trace_op()\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action in the environment and return the next observation, reward, termination, truncation, and info.\n",
    "        \"\"\"\n",
    "        return self.env.step(action)\n",
    "\n",
    "\n",
    "def user_feedback(obs, action, next_obs):\n",
    "    \"\"\"\n",
    "    Provide feedback from the user.\n",
    "    \"\"\"\n",
    "    return f\"Taking action {action.data} at observation {obs['observation'].data} resulted in next observation {next_obs['observation'].data}. Recieved feedback {next_obs['feedback'].data}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space maximum and minimum values are equal. Actual equal coordinates: [(36,), (37,), (38,)]\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/metaworld/policies/policy.py:42: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.expert_action to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.expert_action` for environment variables or `env.get_wrapper_attr('expert_action')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards: 16.13824992438523\n",
      "Success: True\n",
      "Termination: False\n",
      "Truncation: False\n",
      "# of time steps: 7\n"
     ]
    }
   ],
   "source": [
    "# Run the expert policy to get an oracle performance\n",
    "\n",
    "\n",
    "def expert_run(env, horizon):\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Rollout\n",
    "    sum_of_rewards = 0\n",
    "    t = 0\n",
    "    expert_action = None\n",
    "    while t < horizon:\n",
    "        action = env.env.action_space.sample() if expert_action is None else expert_action\n",
    "        next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        expert_action = env.env.expert_action\n",
    "\n",
    "        sum_of_rewards += reward.data  # not traced\n",
    "        t += 1\n",
    "        if termination or truncation or info.data[\"success\"]:\n",
    "            break\n",
    "\n",
    "    print(\"Sum of rewards:\", sum_of_rewards)\n",
    "    print(\"Success:\", info.data[\"success\"])\n",
    "    print(\"Termination:\", termination.data)\n",
    "    print(\"Truncation:\", truncation.data)\n",
    "    print(\"# of time steps:\", t)\n",
    "\n",
    "    return sum_of_rewards, info.data[\"success\"], termination.data, truncation.data\n",
    "\n",
    "\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "sum_of_rewards, success, termination, truncation = expert_run(env, horizon=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_step(env, horizon, user_feedback, controller, optimizer, max_iter=None, delay=0):\n",
    "    \"\"\"Run optimizer step for every time step.\"\"\"\n",
    "\n",
    "    max_iter = max_iter or horizon * 2\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Rollout\n",
    "    sum_of_rewards = 0\n",
    "    t = 0\n",
    "    i = 0\n",
    "    controller_input = obs[\"observation\"]\n",
    "    while t < horizon and i < max_iter:\n",
    "\n",
    "        controller_input = controller_input.detach()  # Detach; otherwise, it would be back-propagated across time.\n",
    "        error = None\n",
    "        try:\n",
    "            for _ in range(delay + 1):  # Delayed feedback\n",
    "                action = controller(controller_input)\n",
    "                next_obs, reward, termination, truncation, info = env.step(action)\n",
    "                controller_input = next_obs[\"observation\"]\n",
    "                sum_of_rewards += reward.data  # not traced\n",
    "                t += 1  # time step\n",
    "            if termination or truncation or info.data[\"success\"]:\n",
    "                break\n",
    "        except trace.TraceExecutionError as e:\n",
    "            error = e\n",
    "\n",
    "        if error is None:\n",
    "            # only get feedback for the last observation\n",
    "            feedback = user_feedback(obs, action, next_obs)  # not traced\n",
    "            obs = next_obs\n",
    "            target = next_obs[\"observation\"]\n",
    "        else:  # Self debugging\n",
    "            feedback = str(error)\n",
    "            target = error.exception_node\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(target, feedback)  # obs = next obs\n",
    "        optimizer.step(verbose=\"output\")\n",
    "        i += 1  # optimization iteration\n",
    "\n",
    "        print(f\"Time Step: {t} of {horizon}\")\n",
    "        print(f\"Iteration: {i}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        print(f\"Variable:\\n {controller.parameter.data}\")\n",
    "\n",
    "    print(\"Termination:\", termination.data)\n",
    "    print(\"Truncation:\", truncation.data)\n",
    "    print(\"Sum of rewards:\", sum_of_rewards)\n",
    "    print(\"Success:\", info.data[\"success\"])\n",
    "    print(\"# of optimization iterations:\", i)\n",
    "    print(\"# of time steps:\", t)\n",
    "\n",
    "    return sum_of_rewards, info.data[\"success\"], termination.data, truncation.data, optimizer\n",
    "\n",
    "\n",
    "@trace.trace_op()\n",
    "def parse_obs(obs):\n",
    "    \"\"\"Parse the observation string into a dictionary of lists of floas.\"\"\"\n",
    "    import json\n",
    "\n",
    "    obs = json.loads(obs)\n",
    "    for key in obs:\n",
    "        obs[key] = obs[key].replace(\"[\", \"\").replace(\"]\", \"\").split()\n",
    "        obs[key] = [float(i) for i in obs[key]]\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given the feedback and its high level of trustworthiness (stepsize = 1), it is clear that the action determined by the controller coded in __code0 is incorrect for achieving the desired goal state, as indicated by the provided feedback. This feedback suggests that the action [-0.07 0.68 0.12 0.] would have been more promising. The current controller function `def controller(obs: str): return action_space.sample()` appears to randomly sample an action from the action space, which in this context has resulted in a non-efficient action. To align with the feedback, a more deterministic or goal-directed action selection method is required, potentially one that could move towards the suggested promising action of [-0.07 0.68 0.12 0.]. Although the exact implementation would heavily depend on the available methods to interact with or analyze the `action_space`, a basic deterministic approach could be to return a fixed action that aligns more closely with the provided feedback.\",\n",
      " \"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs: str):\\n    return [-0.07, 0.68, 0.12, 0.]\"\n",
      "}\n",
      "}\n",
      "Time Step: 2 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [0.62654048 0.82551115 0.21327155 0.45899312] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[0.358 0.554 0.189]\", \"gripper_distance_apart\": \"0.431\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.055e-04  1.445e-04  1.938e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 3.505e-01  5.446e-01  1.818e-01  4.715e-01 -6.380e-02  6.788e-01\\n  1.941e-02  2.736e-04 -8.266e-04  2.046e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You've received a reward of 0.003598908571426461. You've moved the wrong way. You are presently farther from the goal than before. Action [-0.07  0.68  0.12  0.  ] is promising..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [-0.07, 0.68, 0.12, 0.]\n",
      "LLM response:\n",
      " {\n",
      "\t\"reasoning\": \"The provided feedback indicates that the chosen action [-0.07, 0.68, 0.12, 0.0] based on the observed state has led to positive progress, as evidenced by the received reward and the suggestion to continue with the same action. Since the feedback does not suggest a need for changing the action or indicate any issues with the observed outcomes, and given the stepsize of 1 which suggests full trust in this feedback, there doesn't seem to be a necessity to alter the values of the variables, especially the action defined in __code0. The controller function defined in __code0, which generates the action based on the observed state, seems to be performing as expected in aligning with the desired outcomes.\",\n",
      "\t\"suggestion\": {}\n",
      "}\n",
      "  \n",
      " \n",
      "Cannot extract suggestion from LLM's response: \n",
      "{\n",
      "\t\"reasoning\": \"The provided feedback indicates that the chosen action [-0.07, 0.68, 0.12, 0.0] based on the observed state has led to positive progress, as evidenced by the received reward and the suggestion to continue with the same action. Since the feedback does not suggest a need for changing the action or indicate any issues with the observed outcomes, and given the stepsize of 1 which suggests full trust in this feedback, there doesn't seem to be a necessity to alter the values of the variables, especially the action defined in __code0. The controller function defined in __code0, which generates the action based on the observed state, seems to be performing as expected in aligning with the desired outcomes.\",\n",
      "\t\"suggestion\": {}\n",
      "}\n",
      "  \n",
      " \n",
      "Time Step: 4 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[0.358 0.554 0.189]\", \"gripper_distance_apart\": \"0.431\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.055e-04  1.445e-04  1.938e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 3.505e-01  5.446e-01  1.818e-01  4.715e-01 -6.380e-02  6.788e-01\\n  1.941e-02  2.736e-04 -8.266e-04  2.046e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[0.029 0.684 0.12 ]\", \"gripper_distance_apart\": \"0.387\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.591e-04  2.772e-04  1.658e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 3.943e-02  6.855e-01  1.196e-01  3.841e-01 -6.379e-02  6.788e-01\\n  1.941e-02  4.669e-04 -4.855e-04  1.631e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You've earned a reward of 0.053216858411591954. You're heading in the right direction. Consider going to [-0.07  0.68  0.12  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [-0.07, 0.68, 0.12, 0.]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback highlights that the chosen action sequence of [-0.07, 0.68, 0.12, 0.0] is heading in the right direction, but suggests an adjustment to the action pose to [-0.07, 0.68, 0.05, 0.0]. This indicates that the third value in the action list, which represents a position or movement in the z-axis (assuming a 3D space from context), should be changed from 0.12 to 0.05 for better performance or to align closer with expected behavior. Since the stepsize is 1, indicating full trust in the feedback, we follow the recommendation precisely and adjust the controller function in __code0 to return the suggested action of [-0.07, 0.68, 0.05, 0.0].\",\n",
      " \"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs: str):\\\\n    return [-0.07, 0.68, 0.05, 0.]\"\n",
      "}\n",
      "}\n",
      "Time Step: 6 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[0.029 0.684 0.12 ]\", \"gripper_distance_apart\": \"0.387\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.591e-04  2.772e-04  1.658e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 3.943e-02  6.855e-01  1.196e-01  3.841e-01 -6.379e-02  6.788e-01\\n  1.941e-02  4.669e-04 -4.855e-04  1.631e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07   0.681  0.12 ]\", \"gripper_distance_apart\": \"0.502\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.603e-04 -2.328e-04  4.335e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.958e-02  6.808e-01  1.199e-01  4.996e-01 -6.377e-02  6.789e-01\\n  1.942e-02 -5.242e-05 -8.192e-05  4.336e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback Your reward=0.09336837941677875. You are getting there! You are moving in the right general direction. Try moving to pose [-0.07  0.68  0.05  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\\n    return [-0.07, 0.68, 0.05, 0.]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given the SyntaxError indicating an 'unexpected character after line continuation character', the error likely resides within the code provided for __code0. Since the exception and feedback both point towards a syntax error related to the use of line continuation or other character issues within the __code0 variable, I will adjust the format of the code defining the controller function to eliminate syntax errors. Proper Python function syntax does not inherently have issues with the characters presented, implying there might be invisible or misused characters not suitable for Python syntax, such as unnecessary line continuation characters or improperly used special characters. The correct approach is to ensure the code is syntactically correct, respecting Python's indentation and character use.\",\n",
      " \"suggestion\": {\"__code0\": \"def controller(obs: str):\\n    return [-0.07, 0.68, 0.05, 0.0]\"}\n",
      "}\n",
      "Time Step: 6 of 30\n",
      "Iteration: 4\n",
      "Feedback: TraceExecutionError: (SyntaxError) unexpected character after line continuation character (<string>, line 1)\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [-0.07, 0.68, 0.05, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback points out that the current action, defined by the controller's output, does not close the gripper, which is crucial for completing the task successfully. The feedback suggests moving to pose [0.04 0.88 0.24 1.0] to both close the gripper and possibly correct the arm's movement direction. Given a stepsize of 1, we fully trust this feedback and therefore must adjust the controller's output to match the suggested action. The current controller output is [-0.07, 0.68, 0.05, 0.0] and needs to be updated to [0.04, 0.88, 0.24, 1.0] to close the gripper and potentially adjust the arm's position to a more favorable state.\",\n",
      "\"suggestion\": {\n",
      "\"__code0\": \"def controller(obs: str):\\n    return [0.04, 0.88, 0.24, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 8 of 30\n",
      "Iteration: 5\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 0.0] at observation {\"hand_pos\": \"[-0.07   0.681  0.12 ]\", \"gripper_distance_apart\": \"0.502\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.603e-04 -2.328e-04  4.335e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.958e-02  6.808e-01  1.199e-01  4.996e-01 -6.377e-02  6.789e-01\\n  1.942e-02 -5.242e-05 -8.192e-05  4.336e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.075  0.675  0.063]\", \"gripper_distance_apart\": \"0.602\", \"puck_pos\": \"[-0.064  0.68   0.019]\", \"puck_rot\": \"[-1.018e-03  3.712e-04 -1.747e-03  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.396e-02  6.748e-01  6.715e-02  5.989e-01 -6.435e-02  6.797e-01\\n  1.798e-02  7.553e-04  8.050e-04 -1.743e-03  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback Alright, you just earned 0.11843781161021524 reward units. Closing the gripper at this point is a must. The latest arm movement was in a wrong direction. Finishing the task is now more distant than previously.Also, closing the gripper at this point is a must. Try moving to pose [0.04 0.88 0.24 1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [0.04, 0.88, 0.24, 1.0]\n",
      "Termination: False\n",
      "Truncation: False\n",
      "Sum of rewards: 13.321032040228076\n",
      "Success: True\n",
      "# of optimization iterations: 5\n",
      "# of time steps: 10\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer (Delay=1)\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "action_space = env.env.action_space\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs: str):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "max_iter = None\n",
    "delay = 1\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env,\n",
    "    horizon=horizon,\n",
    "    controller=controller,\n",
    "    user_feedback=user_feedback,\n",
    "    optimizer=optimizer,\n",
    "    max_iter=max_iter,\n",
    "    delay=delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback suggests that the action resulting from the controller logic is leading the arm movement in the wrong direction, thus making the task of reaching the goal more distant than before. The current implementation of the controller does not take into account the desired next action as per feedback, which specifically recommends moving to [-0.07  0.68  0.12  0.  ]. Therefore, the controller logic needs to be hardcoded to return this recommended action sequence instead of using action_space.sample(), to ensure the arm moves in the correct direction towards the goal.\",\n",
      "\"suggestion\": {\n",
      "    \"__code1\": \"def controller(obs: str):\\n    return [-0.07, 0.68, 0.12, 0.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 3 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [ 0.72635784  0.08292244 -0.40057622 -0.15462556] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[0.461 0.436 0.296]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.757e-04 -3.166e-04  1.883e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.512e-01  4.467e-01  3.063e-01  1.000e+00 -6.375e-02  6.789e-01\\n  1.941e-02 -3.537e-04  6.340e-04  1.888e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The previous step's reward was 0.002834649069414327. The latest arm movement was in a wrong direction. Finishing the task is now more distant than previously. Moving to [-0.07  0.68  0.12  0.  ] now is a good idea..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [-0.07, 0.68, 0.12, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback suggests an adjustment in the controller's return action, specifically changing the third value from 0.12 to 0.05 to align with the recommendation 'Try moving to pose [-0.07  0.68, 0.05, 0.0].' Given that the stepsize is 1, this feedback is completely trusted and suggests a direct modification to the action produced by the controller function. This change aims to improve the outcome by moving in the direction that the feedback identified as promising.\",\n",
      "\"suggestion\": {\n",
      "    \"__code1\": \"def controller(obs: str):\\n    return [-0.07, 0.68, 0.05, 0.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 6 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[0.461 0.436 0.296]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.757e-04 -3.166e-04  1.883e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.512e-01  4.467e-01  3.063e-01  1.000e+00 -6.375e-02  6.789e-01\\n  1.941e-02 -3.537e-04  6.340e-04  1.888e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.053  0.681  0.121]\", \"gripper_distance_apart\": \"0.999\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.175e-04  3.500e-04  2.535e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-4.960e-02  6.811e-01  1.208e-01  9.995e-01 -6.379e-02  6.788e-01\\n  1.942e-02  3.453e-04 -5.520e-04  2.412e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback Your reward=0.09206725848905543. You are getting there! You are moving in the right general direction. Try moving to pose [-0.07  0.68  0.05  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [-0.07, 0.68, 0.05, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Based on the feedback, it's clear that the expected outcome is to have the gripper closed to achieve the task's goal. This is signaled by the repeated feedback 'But, try closing the gripper. Moving to [-0.07, 0.68, 0.05, 1. ] now is a good idea.'. The current action passed is '[-0.07, 0.68, 0.05, 0.0]' which does not close the gripper, as indicated by the 0 at the end of the action list. To achieve the desired outcome, we need to change this value to 1, which presumably closes the gripper, as suggested in the feedback. Given a stepsize of 1, we fully trust this feedback and thus must adjust the action accordingly.\",\n",
      " \"suggestion\": {\n",
      "    \"__code1\": \"def controller(obs: str):\\n    return [-0.07, 0.68, 0.05, 1.0]\"\n",
      " }\n",
      "}\n",
      "Time Step: 9 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 0.0] at observation {\"hand_pos\": \"[-0.053  0.681  0.121]\", \"gripper_distance_apart\": \"0.999\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.175e-04  3.500e-04  2.535e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-4.960e-02  6.811e-01  1.208e-01  9.995e-01 -6.379e-02  6.788e-01\\n  1.942e-02  3.453e-04 -5.520e-04  2.412e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[3.396e-04 6.934e-05 1.292e-06 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.005e-02  6.804e-01  4.997e-02  9.996e-01 -6.378e-02  6.789e-01\\n  1.941e-02 -6.823e-04 -1.494e-04  1.277e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The reward is 0.11752111289489685. You're getting closer. Keep going!But, try closing the gripper. Try closing the gripper. Moving to [-0.07  0.68  0.05  1.  ] now is a good idea..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [-0.07, 0.68, 0.05, 1.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given the feedback suggests that the current action taken [-0.07, 0.68, 0.05, 1.0] has moved the setup in the wrong direction and recommends a new action [0.04, 0.88, 0.24, 1.0] to move closer to the goal, we need to update the controller function defined in '__code1' to return this recommended action. Since the stepsize is 1, we fully trust this feedback and make the exact recommended change.\",\n",
      " \"suggestion\": {\n",
      "    \"__code1\": \"def controller(obs: str):\\n    return [0.04, 0.88, 0.24, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 12 of 30\n",
      "Iteration: 4\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 1.0] at observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[3.396e-04 6.934e-05 1.292e-06 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.005e-02  6.804e-01  4.997e-02  9.996e-01 -6.378e-02  6.789e-01\\n  1.941e-02 -6.823e-04 -1.494e-04  1.277e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.703\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[2.913e-04 7.093e-05 1.274e-06 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.994e-02  6.801e-01  5.003e-02  7.632e-01 -6.378e-02  6.789e-01\\n  1.941e-02 -7.457e-04 -1.605e-04  1.286e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You've got yourself 0.11754373973268653 units of reward. You moved in the wrong direction. You're now farther away from the goal than before. I recommend that you move to [0.04 0.88 0.24 1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    return [0.04, 0.88, 0.24, 1.0]\n",
      "Termination: False\n",
      "Truncation: False\n",
      "Sum of rewards: 26.740834699967685\n",
      "Success: True\n",
      "# of optimization iterations: 4\n",
      "# of time steps: 15\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer (Delay=2)\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "action_space = env.env.action_space\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs: str):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "max_iter = None\n",
    "delay = 2\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env,\n",
    "    horizon=horizon,\n",
    "    controller=controller,\n",
    "    user_feedback=user_feedback,\n",
    "    optimizer=optimizer,\n",
    "    max_iter=max_iter,\n",
    "    delay=delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\"reasoning\": \"The provided feedback suggests that the action taken by the controller (__code2) led to a positive outcome with a recommendation to continue on the same path but to consider a specific move [-0.07 0.68 0.12 0.]. However, the __code2 as it stands only samples randomly from the action space without considering this feedback or targeting a specific action. To align the controller's action with the feedback, the __code2 function needs to be modified to target or approximate the recommended action. Given the stepsize of 1, this feedback is fully trusted and should be incorporated directly into the action selection process by the controller.\",\n",
      " \"suggestion\": {\"__code2\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Modified to target the recommended action based on feedback.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.12, 0.0]\"}}\n",
      "\n",
      "Time Step: 4 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [-0.72980699  0.44297668  0.05070864 -0.37951625] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[0.037 0.439 0.214]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.498e-04  8.953e-05  1.898e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.700e-02  4.389e-01  2.236e-01  1.000e+00 -6.381e-02  6.788e-01\\n  1.941e-02  2.552e-04 -9.379e-04  2.113e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback 0.010879230012948783 points for you. Your progress is great! Continue down the same path. I recommend that you move to [-0.07  0.68  0.12  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Modified to target the recommended action based on feedback.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.12, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Based on the feedback provided, the actions taken by the controller do not lead to the expected improvement in task completion. Specifically, the feedback suggests changing the third element of the action array from 0.12 to 0.05 in order to get closer to completing the task. As the stepsize is 1, this feedback is fully trusted and will be applied directly to the modification of the __code2 variable, where the controller function defines the action array. Therefore, the value of the third element in the return statement of the controller function needs to be adjusted from 0.12 to 0.05 to align with this feedback.\",\n",
      "\"suggestion\": {\n",
      "\"__code2\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Modified to target the recommended action based on feedback.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.05, 0.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 8 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[0.037 0.439 0.214]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.498e-04  8.953e-05  1.898e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.700e-02  4.389e-01  2.236e-01  1.000e+00 -6.381e-02  6.788e-01\\n  1.941e-02  2.552e-04 -9.379e-04  2.113e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07   0.679  0.12 ]\", \"gripper_distance_apart\": \"0.993\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-5.575e-05  1.501e-04  3.597e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.984e-02  6.795e-01  1.202e-01  9.933e-01 -6.380e-02  6.788e-01\\n  1.941e-02  3.278e-04 -8.160e-04  3.605e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback +0.09310216627582853 points The arm's move direction is off. Compared to the previous state, you are now farther from finishing the task. If you want a tip, [-0.07  0.68  0.05  0.  ] is a good pose to aim for next..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Modified to target the recommended action based on feedback.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.05, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback clearly indicates that the controller's current output action, [-0.07, 0.68, 0.05, 0.0], is moving the agent in the right direction but lacks the final component to close the gripper, as suggested by the desired action [-0.07, 0.68, 0.05, 1. ]. Since the steps of adjustment are fully trusted (stepsize = 1), the action output by the controller (__code2) needs to include the action to close the gripper. Therefore, the fourth element of the action list should be changed from 0.0 to 1.0 to follow the feedback appropriately.\",\n",
      " \"suggestion\": {\n",
      "    \"__code2\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Modified to target the recommended action based on feedback.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.05, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 12 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 0.0] at observation {\"hand_pos\": \"[-0.07   0.679  0.12 ]\", \"gripper_distance_apart\": \"0.993\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-5.575e-05  1.501e-04  3.597e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.984e-02  6.795e-01  1.202e-01  9.933e-01 -6.380e-02  6.788e-01\\n  1.941e-02  3.278e-04 -8.160e-04  3.605e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.995\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 2.592e-04 -2.355e-04  3.098e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.998e-02  6.801e-01  5.001e-02  9.953e-01 -6.378e-02  6.788e-01\\n  1.941e-02  1.469e-04 -1.521e-04  3.004e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You got a reward of 0.11753496573025277. Your latest move has got you closer to finishing your task.But, you should close the gripper. You should close the gripper. You should go to [-0.07  0.68  0.05  1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Modified to target the recommended action based on feedback.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.05, 1.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given that the feedback is fully trusted (stepsize of 1) and suggests moving to the pose [-0.07, 0.68, 0.05, 1.0], but the controller already targets this exact action with no observed improvements, it indicates that modifying __code2 to adjust the controller's action choice may not be the right approach. Instead, considering the feedback is already in line with the actions being taken, the issue may not lie within the action choice but perhaps in the interpretation of the feedback or the actions' effects on the environment. However, without a specific request for a different outcome or a clear indication that an alternative action could yield a better result, altering the targeted pose (action values) seems redundant. Therefore, there appears to be no necessary change to the values in __code2 based on the provided information and feedback.\",\n",
      "\"suggestion\": {}\n",
      "}\n",
      "Cannot extract suggestion from LLM's response: \n",
      "{\n",
      "\"reasoning\": \"Given that the feedback is fully trusted (stepsize of 1) and suggests moving to the pose [-0.07, 0.68, 0.05, 1.0], but the controller already targets this exact action with no observed improvements, it indicates that modifying __code2 to adjust the controller's action choice may not be the right approach. Instead, considering the feedback is already in line with the actions being taken, the issue may not lie within the action choice but perhaps in the interpretation of the feedback or the actions' effects on the environment. However, without a specific request for a different outcome or a clear indication that an alternative action could yield a better result, altering the targeted pose (action values) seems redundant. Therefore, there appears to be no necessary change to the values in __code2 based on the provided information and feedback.\",\n",
      "\"suggestion\": {}\n",
      "}\n",
      "Time Step: 16 of 30\n",
      "Iteration: 4\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 1.0] at observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.995\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 2.592e-04 -2.355e-04  3.098e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.998e-02  6.801e-01  5.001e-02  9.953e-01 -6.378e-02  6.788e-01\\n  1.941e-02  1.469e-04 -1.521e-04  3.004e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.907\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.029e-04  3.288e-04  3.102e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.998e-02  6.800e-01  5.005e-02  9.431e-01 -6.376e-02  6.789e-01\\n  1.942e-02 -1.407e-04  2.128e-04  3.120e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You just got 0.11755111762103321 points. You're heading in the right direction. Aim to reach pose [-0.07  0.68  0.05  1.  ] at the next step..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Modified to target the recommended action based on feedback.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.05, 1.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback suggests that taking action [-0.07, 0.68, 0.05, 1.0] is not optimal and recommends trying action [0.04, 0.88, 0.24, 1.0] instead to improve the reward and progress towards the goal. This means we need to adjust the controller function in __code2 to return the recommended action [0.04, 0.88, 0.24, 1.0]. Since the stepsize is 1, we completely trust the feedback and make the recommended adjustment.\",\n",
      " \"suggestion\": {\n",
      "   \"__code2\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Modified to target the recommended action based on feedback.\\n    \\\"\\\"\\\"\\n    return [0.04, 0.88, 0.24, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 20 of 30\n",
      "Iteration: 5\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 1.0] at observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.907\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.029e-04  3.288e-04  3.102e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.998e-02  6.800e-01  5.005e-02  9.431e-01 -6.376e-02  6.789e-01\\n  1.942e-02 -1.407e-04  2.128e-04  3.120e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.452\", \"puck_pos\": \"[-0.064  0.68   0.021]\", \"puck_rot\": \"[ 2.851e-03 -2.567e-04 -4.439e-05  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.006e-02  6.801e-01  4.987e-02  4.520e-01 -6.376e-02  6.800e-01\\n  2.147e-02 -1.355e-03 -2.357e-04 -1.031e-04  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The previous step's reward was 0.11837554931117567. You are making progress towards achieving the goal. Keep it up! One thing to try is to take action [0.04 0.88 0.24 1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Modified to target the recommended action based on feedback.\n",
      "    \"\"\"\n",
      "    return [0.04, 0.88, 0.24, 1.0]\n",
      "Termination: False\n",
      "Truncation: False\n",
      "Sum of rewards: 37.34044866518296\n",
      "Success: True\n",
      "# of optimization iterations: 5\n",
      "# of time steps: 24\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer (Delay=3)\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "action_space = env.env.action_space\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs: str):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "max_iter = None\n",
    "delay = 3\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env,\n",
    "    horizon=horizon,\n",
    "    controller=controller,\n",
    "    user_feedback=user_feedback,\n",
    "    optimizer=optimizer,\n",
    "    max_iter=max_iter,\n",
    "    delay=delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback indicates that the chosen action, based on the current observation led to poor performance and a minimal reward. Given the significant impact described by the feedback and a stepsize of 1, implying full trust in the feedback, it's clear the action chosen by the 'controller' function in __code3 variable needs to be directed towards the suggested new pose [-0.25 0.61 0.12 0.]. This implies that the controller logic should be redefined to consider the feedback direction to improve the outcomes. The suggested action involves adjusting the geometric positioning, which should influence the 'controller' function to select actions closer to the recommendation.\",\n",
      " \"suggestion\": {\n",
      "   \"__code3\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.25, 0.61, 0.12, 0.].\\n    \\\"\\\"\\\"\\n    return [-0.25, 0.61, 0.12, 0.]\"\n",
      "}\n",
      "}\n",
      "Time Step: 5 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [ 0.75296846 -0.88286393 -0.32776588 -0.69944107] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.109  0.519  0.089]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.248  0.609  0.022]\", \"puck_rot\": \"[-0.207  0.598  0.315  0.707]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.119  0.529  0.098  1.    -0.247  0.61   0.021 -0.221  0.618  0.302\\n  0.691  0.     0.     0.     0.     0.     0.     0.   ]\"}. Recieved feedback 0.0008255661450304791 points for you. Your move direction is off. Getting to your objective will now take longer. My suggestion is that you go towards pose [-0.25  0.61  0.12  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.25, 0.61, 0.12, 0.].\n",
      "    \"\"\"\n",
      "    return [-0.25, 0.61, 0.12, 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/metaworld/policies/policy.py:42: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback indicates that the suggested pose of [-0.25, 0.61, 0.12, 0.0] has led to an undesired direction, making it harder to achieve the task's goal. The feedback further suggests aiming for a new pose, [-0.23, 0.59, 0.12, 0.0], instead. Given the step size of 1, this feedback is completely trusted, and hence, the action suggested by the controller function needs to be adjusted accordingly. Therefore, updating the controller function to return the new suggested pose will align the outcomes closer to the feedback's guidance.\",\n",
      " \"suggestion\": {\n",
      "    \"__code3\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.23, 0.59, 0.12, 0.].\\n    \\\"\\\"\\\"\\n    return [-0.23, 0.59, 0.12, 0.]\"\n",
      "}\n",
      "}\n",
      "Time Step: 10 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.25, 0.61, 0.12, 0.0] at observation {\"hand_pos\": \"[-0.109  0.519  0.089]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.248  0.609  0.022]\", \"puck_rot\": \"[-0.207  0.598  0.315  0.707]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.119  0.529  0.098  1.    -0.247  0.61   0.021 -0.221  0.618  0.302\\n  0.691  0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.25  0.61  0.12]\", \"gripper_distance_apart\": \"0.998\", \"puck_pos\": \"[-0.226  0.591  0.019]\", \"puck_rot\": \"[0.248 0.663 0.616 0.346]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.25   0.61   0.12   0.998 -0.227  0.591  0.019  0.243  0.663  0.615\\n  0.351  0.     0.     0.     0.     0.     0.     0.   ]\"}. Recieved feedback Your reward for the latest step is 0.026917349971956987. You've vectored the arm in a direction that will make it more difficult to accomplish your task. I suggest you move to pose [-0.23  0.59  0.12  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.23, 0.59, 0.12, 0.].\n",
      "    \"\"\"\n",
      "    return [-0.23, 0.59, 0.12, 0.]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Analyzing the feedback, it's clear that the action [-0.23, 0.59, 0.12, 0.0] led to a situation where the system suggests that moving closer to [-0.22, 0.58, 0.05, 0.0] would have been a more promising strategy. This implies that the controller should be adjusted to suggest actions more in line with what the feedback indicates, especially targeting a more optimal z-coordinate of 0.05 as suggested multiple times in the feedback.\",\n",
      " \"suggestion\": {\n",
      "    \"__code3\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.22, 0.58, 0.05, 0.].\\n    \\\"\\\"\\\"\\n    return [-0.22, 0.58, 0.05, 0.]\"\n",
      " }\n",
      "}\n",
      "Time Step: 15 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.23, 0.59, 0.12, 0.0] at observation {\"hand_pos\": \"[-0.25  0.61  0.12]\", \"gripper_distance_apart\": \"0.998\", \"puck_pos\": \"[-0.226  0.591  0.019]\", \"puck_rot\": \"[0.248 0.663 0.616 0.346]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.25   0.61   0.12   0.998 -0.227  0.591  0.019  0.243  0.663  0.615\\n  0.351  0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.23  0.59  0.12]\", \"gripper_distance_apart\": \"0.999\", \"puck_pos\": \"[-0.215  0.578  0.019]\", \"puck_rot\": \"[0.501 0.5   0.704 0.059]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.23   0.59   0.12   0.999 -0.216  0.578  0.019  0.497  0.502  0.705\\n  0.064  0.     0.     0.     0.     0.     0.     0.   ]\"}. Recieved feedback You've received a reward of 0.027006347079246096. Your latest action has taken you further away from completing your task. Moving to [-0.22  0.58  0.05  0.  ] now is a good idea..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.22, 0.58, 0.05, 0.].\n",
      "    \"\"\"\n",
      "    return [-0.22, 0.58, 0.05, 0.]\n",
      "LLM response:\n",
      " {\"reasoning\": \"Based on the feedback provided, the final action taken by the controller is correct in terms of the position it aims to achieve but lacks the gripper closure component, which is indicated by the last element of the action array being 0 instead of 1. The feedback suggests the necessity to close the gripper to accomplish the task successfully, indicated by the advice to assume the pose [-0.22, 0.58, 0.05, 1.]. Therefore, modifying the controller to include closing the gripper (changing the last element of the return array from 0 to 1) in the action will better align the outcome with the desired feedback, fully trusting the feedback with a stepsize of 1.\",\n",
      " \"suggestion\": {\"__code3\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.22, 0.58, 0.05, 1.].\\n    \\\"\\\"\\\"\\n    return [-0.22, 0.58, 0.05, 1.]\"}\n",
      "}\n",
      "Time Step: 20 of 30\n",
      "Iteration: 4\n",
      "Feedback: Taking action [-0.22, 0.58, 0.05, 0.0] at observation {\"hand_pos\": \"[-0.23  0.59  0.12]\", \"gripper_distance_apart\": \"0.999\", \"puck_pos\": \"[-0.215  0.578  0.019]\", \"puck_rot\": \"[0.501 0.5   0.704 0.059]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.23   0.59   0.12   0.999 -0.216  0.578  0.019  0.497  0.502  0.705\\n  0.064  0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.22  0.58  0.05]\", \"gripper_distance_apart\": \"0.999\", \"puck_pos\": \"[-0.205  0.565  0.019]\", \"puck_rot\": \"[ 0.654  0.266  0.673 -0.219]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.22   0.58   0.05   0.999 -0.205  0.566  0.019  0.654  0.27   0.673\\n -0.215  0.     0.     0.     0.     0.     0.     0.   ]\"}. Recieved feedback You've received a reward of 0.02858794488819009. You're heading in the right direction.But, you have to close the gripper. You have to close the gripper. I would try action [-0.22  0.58  0.05  1.  ] if I were you..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action aimed towards the suggested pose [-0.22, 0.58, 0.05, 1.].\n",
      "    \"\"\"\n",
      "    return [-0.22, 0.58, 0.05, 1.]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Based on the feedback, it's clear that the evaluated controller action [-0.22, 0.58, 0.05, 1.0] needs to be adjusted towards the suggested pose [0.04, 0.88, 0.24, 1.0] to enhance the reward and properly direct towards the goal. Given that the steps size is 1, this suggests full trust in this feedback, meaning the suggestion must be implemented to its fullest extent to ensure the result aligns with expectations. Therefore, the controller’s logic in __code3 should updated to return this specified action [0.04, 0.88, 0.24, 1.0] instead of its current output. This alteration should provide the necessary correction towards achieving the goal as indicated by the feedback.\",\n",
      "\"suggestion\": {\n",
      "\"__code3\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action aimed towards the suggested pose [0.04, 0.88, 0.24, 1.0].\\n    \\\"\\\"\\\"\\n    return [0.04, 0.88, 0.24, 1.]\"\n",
      "}\n",
      "}\n",
      "Time Step: 25 of 30\n",
      "Iteration: 5\n",
      "Feedback: Taking action [-0.22, 0.58, 0.05, 1.0] at observation {\"hand_pos\": \"[-0.22  0.58  0.05]\", \"gripper_distance_apart\": \"0.999\", \"puck_pos\": \"[-0.205  0.565  0.019]\", \"puck_rot\": \"[ 0.654  0.266  0.673 -0.219]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.22   0.58   0.05   0.999 -0.205  0.566  0.019  0.654  0.27   0.673\\n -0.215  0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.22  0.58  0.05]\", \"gripper_distance_apart\": \"0.460\", \"puck_pos\": \"[-0.203  0.579  0.02 ]\", \"puck_rot\": \"[0.707 0.023 0.707 0.023]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.22   0.58   0.05   0.46  -0.203  0.579  0.02   0.707  0.021  0.707\\n  0.024  0.     0.     0.     0.     0.     0.     0.   ]\"}. Recieved feedback The reward is 0.061480964470963975. Press on! You've moving in the right direction. I suggest you move to pose [0.04 0.88 0.24 1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action aimed towards the suggested pose [0.04, 0.88, 0.24, 1.0].\n",
      "    \"\"\"\n",
      "    return [0.04, 0.88, 0.24, 1.]\n",
      "Termination: False\n",
      "Truncation: True\n",
      "Sum of rewards: 44.033197939426806\n",
      "Success: True\n",
      "# of optimization iterations: 5\n",
      "# of time steps: 30\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer (Delay=4)\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "action_space = env.env.action_space\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs: str):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "max_iter = None\n",
    "delay = 4\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env,\n",
    "    horizon=horizon,\n",
    "    controller=controller,\n",
    "    user_feedback=user_feedback,\n",
    "    optimizer=optimizer,\n",
    "    max_iter=max_iter,\n",
    "    delay=delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space maximum and minimum values are equal. Actual equal coordinates: [(36,), (37,), (38,)]\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\"reasoning\": \"The feedback indicates that the current action taken based on the observation led to a positive result, signifying that the arm is moving in the correct direction as per the task's requirements. However, the next suggested action to take involves reaching a specific pose [-0.02 0.72 0.13 0. ]. Since the feedback is entirely trusted (stepsize of 1), and there's a specific pose suggested for the next step, the necessary change must optimize the controller function to achieve this pose. Given the limitations of the problem statement, and considering we cannot change the __code4 directly as per instructions, but instead work with given variables, it looks like we are supposed to understand the feedback in terms of whether our adjustments are correct. The __code4 function 'controller' should ideally be strategized to follow the feedback's advice more closely. However, since we are asked not to change the code but only the value of variables, and given our limitations, we actually can't adjust variables to directly affect the outcome desired by the feedback. The task requirement is a bit of a paradox because adjustments needed to reach the next step pose [-0.02 0.72 0.13 0. ] should be done in the controller function, which requires code change, not just variable adjustment. Thus, without the ability to directly alter the behavior of '__code4', or without specific variables indicated for adjustment, no direct action can be taken based on the instructions provided.\",\n",
      " \"suggestion\": {}\n",
      "}\n",
      "Cannot extract suggestion from LLM's response: \n",
      "{\"reasoning\": \"The feedback indicates that the current action taken based on the observation led to a positive result, signifying that the arm is moving in the correct direction as per the task's requirements. However, the next suggested action to take involves reaching a specific pose [-0.02 0.72 0.13 0. ]. Since the feedback is entirely trusted (stepsize of 1), and there's a specific pose suggested for the next step, the necessary change must optimize the controller function to achieve this pose. Given the limitations of the problem statement, and considering we cannot change the __code4 directly as per instructions, but instead work with given variables, it looks like we are supposed to understand the feedback in terms of whether our adjustments are correct. The __code4 function 'controller' should ideally be strategized to follow the feedback's advice more closely. However, since we are asked not to change the code but only the value of variables, and given our limitations, we actually can't adjust variables to directly affect the outcome desired by the feedback. The task requirement is a bit of a paradox because adjustments needed to reach the next step pose [-0.02 0.72 0.13 0. ] should be done in the controller function, which requires code change, not just variable adjustment. Thus, without the ability to directly alter the behavior of '__code4', or without specific variables indicated for adjustment, no direct action can be taken based on the instructions provided.\",\n",
      " \"suggestion\": {}\n",
      "}\n",
      "Time Step: 6 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [ 0.2404269   0.99019301  0.89788735 -0.07990972] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[0.052 0.742 0.213]\", \"gripper_distance_apart\": \"0.760\", \"puck_pos\": \"[-0.014  0.72   0.026]\", \"puck_rot\": \"[-0.508  0.669 -0.031  0.542]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.042  0.734  0.203  0.75  -0.015  0.719  0.025 -0.494  0.655 -0.029\\n  0.572  0.     0.     0.     0.     0.     0.     0.   ]\"}. Recieved feedback You just got 0.021960193948091417 points. You are on the right track. Keep it up! Aim to reach pose [-0.02  0.72  0.13  0.  ] at the next step..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    \"\"\"\n",
      "    return action_space.sample()\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The repeated feedback emphasizes the necessity to open the gripper, which is not reflected in the actions selected by the controller code `__code4`. The controller function currently returns `action_space.sample()`, which is random and does not consider the specific feedback given. To address this, we need to adjust the controller's code to make sure the action it selects aligns with the feedback, explicitly to open the gripper and move towards the suggested positions. As the stepsize is 1, we fully trust the feedback and hence need to implement this suggestion directly.\",\n",
      "\"suggestion\": {\n",
      "    \"__code4\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action that opens the gripper and moves towards [0.   0.74 0.12 0.  ].\\n    \\\"\\\"\\\"\\n    return [0., 0.74, 0.12, 0.]\"\n",
      "}\n",
      "}\n",
      "Time Step: 12 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.04002415 -0.53525416  0.60376116  0.84706032] at observation {\"hand_pos\": \"[0.052 0.742 0.213]\", \"gripper_distance_apart\": \"0.760\", \"puck_pos\": \"[-0.014  0.72   0.026]\", \"puck_rot\": \"[-0.508  0.669 -0.031  0.542]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.042  0.734  0.203  0.75  -0.015  0.719  0.025 -0.494  0.655 -0.029\\n  0.572  0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.335  0.695  0.499]\", \"gripper_distance_apart\": \"0.297\", \"puck_pos\": \"[0.009 0.74  0.023]\", \"puck_rot\": \"[ 0.78  -0.622  0.066  0.031]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.342  0.699  0.495  0.297  0.008  0.74   0.023  0.777 -0.622  0.092\\n  0.016  0.     0.     0.     0.     0.     0.     0.   ]\"}. Recieved feedback +0.002822345571544699 reward Press on! You've moving in the right direction.But, you should open the gripper. You should open the gripper. I suggest you move to pose [0.   0.74 0.12 0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action that opens the gripper and moves towards [0.   0.74 0.12 0.  ].\n",
      "    \"\"\"\n",
      "    return [0., 0.74, 0.12, 0.]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"According to the feedback, the action suggested by the controller code, [0.0, 0.74, 0.12, 0.0], has been leading the agent in the wrong direction, increasing the distance to the target. The feedback explicitly suggests a new action, [0.0, 0.74, 0.05, 0.0], indicating adjusting the third value in the array (which seems to be related to the vertical movement or positioning of the agent) from 0.12 to 0.05 could potentially improve the performance towards reaching the goal. To align the controller code with the feedback, we need to adjust the return values of the `controller` function in `__code4` to reflect the new action suggested by the feedback. As the stepsize is 1, we fully trust the feedback and thus make a direct adjustment based on it.\",\n",
      "\"suggestion\": {\n",
      "    \"__code4\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action that opens the gripper and moves towards [0.   0.74 0.05 0.  ].\\n    \\\"\\\"\\\"\\n    return [0., 0.74, 0.05, 0.]\"\n",
      "}\n",
      "}\n",
      "Time Step: 18 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [0.0, 0.74, 0.12, 0.0] at observation {\"hand_pos\": \"[-0.335  0.695  0.499]\", \"gripper_distance_apart\": \"0.297\", \"puck_pos\": \"[0.009 0.74  0.023]\", \"puck_rot\": \"[ 0.78  -0.622  0.066  0.031]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-0.342  0.699  0.495  0.297  0.008  0.74   0.023  0.777 -0.622  0.092\\n  0.016  0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[5.96e-05 7.40e-01 1.20e-01]\", \"gripper_distance_apart\": \"0.556\", \"puck_pos\": \"[0.008 0.736 0.022]\", \"puck_rot\": \"[ 0.869 -0.491  0.038 -0.041]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 8.520e-05  7.400e-01  1.200e-01  5.537e-01  8.597e-03  7.348e-01\\n  2.151e-02  8.686e-01 -4.925e-01  1.268e-02 -5.202e-02  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You got a reward of 0.1201130275917304. You've gone in the incorrect direction. Your distance to the target has increased. Action [0.   0.74 0.05 0.  ] is promising..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action that opens the gripper and moves towards [0.   0.74 0.05 0.  ].\n",
      "    \"\"\"\n",
      "    return [0., 0.74, 0.05, 0.]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback suggests that while the robot's movement appears to be in the correct direction, it fails to complete the task because the gripper remains open (last number in the action sequence is 0.0, which suggests the gripper is not closing). The feedback specifically mentions the action it would try, which involves closing the gripper (changing the last number of the action sequence to 1.0) and slightly adjusting the position. Therefore, to follow the feedback completely, the controller function should change its return value to include closing the gripper and adjusting the position as suggested.\",\n",
      "\"suggestion\": {\n",
      "    \"__code4\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action that opens the gripper and moves towards [0.04, 0.88, 0.24, 1.0], effectively closing the gripper as per instructed feedback.\\n    \\\"\\\"\\\"\\n    return [0.04, 0.88, 0.24, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 24 of 30\n",
      "Iteration: 4\n",
      "Feedback: Taking action [0.0, 0.74, 0.05, 0.0] at observation {\"hand_pos\": \"[5.96e-05 7.40e-01 1.20e-01]\", \"gripper_distance_apart\": \"0.556\", \"puck_pos\": \"[0.008 0.736 0.022]\", \"puck_rot\": \"[ 0.869 -0.491  0.038 -0.041]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 8.520e-05  7.400e-01  1.200e-01  5.537e-01  8.597e-03  7.348e-01\\n  2.151e-02  8.686e-01 -4.925e-01  1.268e-02 -5.202e-02  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-4.351e-05  7.400e-01  5.006e-02]\", \"gripper_distance_apart\": \"0.735\", \"puck_pos\": \"[0.011 0.738 0.019]\", \"puck_rot\": \"[ 8.793e-01 -4.763e-01 -7.284e-04 -1.917e-03]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-4.486e-05  7.401e-01  5.003e-02  7.337e-01  1.115e-02  7.383e-01\\n  1.950e-02  8.793e-01 -4.764e-01 -1.204e-03  1.788e-03  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback Your reward gain is 0.15773395892483638. You are on the correct path.But, consider closing the gripper. Consider closing the gripper. My suggestion is that you go towards pose [-4.35e-05  7.40e-01  5.01e-02  1.00e+00]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action that opens the gripper and moves towards [0.04, 0.88, 0.24, 1.0], effectively closing the gripper as per instructed feedback.\n",
      "    \"\"\"\n",
      "    return [0.04, 0.88, 0.24, 1.0]\n",
      "Termination: False\n",
      "Truncation: True\n",
      "Sum of rewards: 42.88197885227602\n",
      "Success: True\n",
      "# of optimization iterations: 4\n",
      "# of time steps: 30\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer (Delay=5)\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "action_space = env.env.action_space\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs: str):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "max_iter = None\n",
    "delay = 5\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env,\n",
    "    horizon=horizon,\n",
    "    controller=controller,\n",
    "    user_feedback=user_feedback,\n",
    "    optimizer=optimizer,\n",
    "    max_iter=max_iter,\n",
    "    delay=delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "  \"reasoning\": \"The feedback suggests that the action taken by the controller did not yield optimal results but did get the system closer to the task's completion. The constant advice throughout feedback is to adjust the pose towards [-0.07  0.68  0.12  0.  ], indicating that the controller's output action should ideally be mapped closer to these target values to improve task performance. Given that __code5 defines the controller's behavior, the controller's logic needs alteration to generate actions closer to the recommended pose. Since the feedback's stepsize is 1, we fully trust and apply this feedback, suggesting that the controller's method of determining actions likely needs a more targeted approach towards achieving the goal pose.\",\n",
      "  \"suggestion\": {\n",
      "    \"__code5\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjust the controller to aim for specific pose values.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.12, 0.0]\"\n",
      "  }\n",
      "}\n",
      "Time Step: 7 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [-0.75089059 -0.42333849  0.17224613  0.108181  ] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.494  0.399  0.334]\", \"gripper_distance_apart\": \"0.866\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[3.050e-04 7.011e-05 7.643e-07 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-4.930e-01  3.989e-01  3.445e-01  8.755e-01 -6.377e-02  6.789e-01\\n  1.941e-02 -7.367e-04 -1.293e-04  7.934e-07  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The reward your latest step earned you is 0.003514826808478401. Your latest move has got you closer to finishing your task. I urge you to move to pose [-0.07  0.68  0.12  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjust the controller to aim for specific pose values.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.12, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback indicates that the current action [-0.07, 0.68, 0.12, 0.0] is making good progress but suggests that changing the third value of the action from 0.12 to 0.05 could lead to better results. Since the stepsize is 1, we completely trust the feedback and adjust the controller function in the '__code5' variable accordingly to produce the suggested pose. This adjustment aims to optimize the output according to the provided feedback.\",\n",
      "\"suggestion\": {\n",
      "    \"__code5\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjust the controller to aim for specific pose values.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.05, 0.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 14 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[-0.494  0.399  0.334]\", \"gripper_distance_apart\": \"0.866\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[3.050e-04 7.011e-05 7.643e-07 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-4.930e-01  3.989e-01  3.445e-01  8.755e-01 -6.377e-02  6.789e-01\\n  1.941e-02 -7.367e-04 -1.293e-04  7.934e-07  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.12]\", \"gripper_distance_apart\": \"0.865\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[4.805e-04 1.749e-04 1.456e-06 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.000e-02  6.800e-01  1.199e-01  8.645e-01 -6.377e-02  6.788e-01\\n  1.941e-02  2.992e-04  9.054e-05  1.382e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback Your reward for the latest step is 0.09329479079936917. You are making good progress. I urge you to move to pose [-0.07  0.68  0.05  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjust the controller to aim for specific pose values.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.05, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Based on the feedback, the action taken by the controller led to a positive step towards completing the task, but it's specifically instructed to close the gripper, which wasn't done. The current action controlled by the __code5 function leads to [-0.07, 0.68, 0.05, 0.0]. According to the feedback, instead of taking an action with the last parameter as 0.0, it's suggested to change the last parameter to 1.0 to close the gripper. Therefore, the __code5 function needs to be updated to return [-0.07, 0.68, 0.05, 1.0], adhering to the stepsize of 1, which indicates full trust in the feedback.\",\n",
      " \"suggestion\": {\n",
      "    \"__code5\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjust the controller to aim for specific pose values including closing the gripper.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.05, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 21 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 0.0] at observation {\"hand_pos\": \"[-0.07  0.68  0.12]\", \"gripper_distance_apart\": \"0.865\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[4.805e-04 1.749e-04 1.456e-06 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.000e-02  6.800e-01  1.199e-01  8.645e-01 -6.377e-02  6.788e-01\\n  1.941e-02  2.992e-04  9.054e-05  1.382e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.907\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[3.523e-04 7.380e-05 2.127e-06 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.997e-02  6.800e-01  5.004e-02  9.065e-01 -6.378e-02  6.789e-01\\n  1.941e-02 -6.755e-04 -1.853e-04  2.077e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback Your reward for the latest step is 0.11754177830834131. You are moving the right way.But, keep the gripper closed. Keep the gripper closed. I suggest you move to pose [-0.07  0.68  0.05  1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjust the controller to aim for specific pose values including closing the gripper.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.05, 1.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback suggests that the current action provided by the controller [-0.07, 0.68, 0.05, 1.0] should be changed to [0.04, 0.88, 0.24, 1.0] to achieve better results and align with the advised action. Since the stepsize is 1, the feedback should be fully trusted and the suggested changes should be made accordingly.\",\n",
      "\"suggestion\": {\n",
      "\"__code5\": \"def controller(obs: str):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjust the controller to aim for specific pose values including closing the gripper.\\n    \\\"\\\"\\\"\\n    return [0.04, 0.88, 0.24, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 28 of 30\n",
      "Iteration: 4\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 1.0] at observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.907\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[3.523e-04 7.380e-05 2.127e-06 1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.997e-02  6.800e-01  5.004e-02  9.065e-01 -6.378e-02  6.789e-01\\n  1.941e-02 -6.755e-04 -1.853e-04  2.077e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.452\", \"puck_pos\": \"[-0.064  0.68   0.022]\", \"puck_rot\": \"[ 2.232e-03 -3.230e-04 -1.874e-04  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.007e-02  6.800e-01  4.995e-02  4.518e-01 -6.376e-02  6.800e-01\\n  2.162e-02  3.748e-03 -3.200e-04 -2.520e-04  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The reward is 0.11838950126288177. You are on the right track. Keep it up! I advise you to take action [0.04 0.88 0.24 1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjust the controller to aim for specific pose values including closing the gripper.\n",
      "    \"\"\"\n",
      "    return [0.04, 0.88, 0.24, 1.0]\n",
      "Termination: False\n",
      "Truncation: True\n",
      "Sum of rewards: 67.9631942990313\n",
      "Success: True\n",
      "# of optimization iterations: 4\n",
      "# of time steps: 35\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer (Delay=6)\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "action_space = env.env.action_space\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs: str):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "max_iter = None\n",
    "delay = 6\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env,\n",
    "    horizon=horizon,\n",
    "    controller=controller,\n",
    "    user_feedback=user_feedback,\n",
    "    optimizer=optimizer,\n",
    "    max_iter=max_iter,\n",
    "    delay=delay,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback describes that the action taken based on the controller's decision led the arm away from completing its task, moving in the wrong direction compared to the desired pose [-0.07  0.68  0.12  0.  ]. Given the trust level (Stepsize) of 1, we completely trust this feedback and should adjust the controller code to reflect the suggested pose. However, since the controller function's implementation (`controller(obs: str)`) isn't directly shown or modifiable in inputs, it's interpreted that the controller's decision-making logic should lead towards this pose. Assuming action space sampling (`action_space.sample()`) is pseudo-random and leads to unsatisfactory outcomes, switching to a deterministic or target-driven strategy would be ideal, preferably aiming directly for the suggested coordinates. As such, adjusting the controller function to consider the desired pose may influence the actions generated towards achieving the goal more effectively.\",\n",
      " \"suggestion\": {\n",
      "    \"__code7\": \"def controller(obs: str):\\n    '''\\n    The controller takes in an observation and returns an action.\\n    '''\\n    return [-0.07, 0.68, 0.12, 0.  ]\"\n",
      "}\n",
      "}\n",
      "Time Step: 8 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [ 0.15537579  0.20447835  0.92484619 -0.85546947] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[-0.172  0.392  0.487]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.172e-05 -2.078e-04  4.515e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-1.816e-01  3.923e-01  4.839e-01  1.000e+00 -6.376e-02  6.788e-01\\n  1.941e-02  1.363e-04  2.737e-04  4.601e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback +0.004445680583838225 points The arm's move direction is off. Compared to the previous state, you are now farther from finishing the task. If you want a tip, [-0.07  0.68  0.12  0.  ] is a good pose to aim for next..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    '''\n",
      "    The controller takes in an observation and returns an action.\n",
      "    '''\n",
      "    return [-0.07, 0.68, 0.12, 0.  ]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback suggests that despite the progress being made with the current action [-0.07, 0.68, 0.12, 0.0], a modification towards [-0.07, 0.68, 0.05, 0.0] is recommended to potentially increase the reward and get closer to completing the task. Given that the stepsize is 1, this feedback should be fully trusted and applied directly. Therefore, changing the third value of the action from 0.12 to 0.05, while keeping the rest unchanged, aligns with the feedback.\",\n",
      "\"suggestion\": {\n",
      "    \"__code7\": \"def controller(obs: str):\\n    '''\\n    The controller takes in an observation and returns an action.\\n    '''\\n    return [-0.07, 0.68, 0.05, 0.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 16 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[-0.172  0.392  0.487]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.172e-05 -2.078e-04  4.515e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-1.816e-01  3.923e-01  4.839e-01  1.000e+00 -6.376e-02  6.788e-01\\n  1.941e-02  1.363e-04  2.737e-04  4.601e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.12]\", \"gripper_distance_apart\": \"0.998\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.345e-04 -2.555e-04  1.488e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.999e-02  6.799e-01  1.200e-01  9.980e-01 -6.378e-02  6.788e-01\\n  1.942e-02  1.417e-04 -3.371e-04  1.890e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The latest step brought you 0.09327502094261125 reward units. You're getting closer. Keep going! You should go to [-0.07  0.68  0.05  0.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    '''\n",
      "    The controller takes in an observation and returns an action.\n",
      "    '''\n",
      "    return [-0.07, 0.68, 0.05, 0.0]\n",
      "LLM response:\n",
      " {\"reasoning\": \"The feedback consistently points out that the gripper should be closed to achieve a better result, with a suggested action of [-0.07, 0.68, 0.05, 1.0]. The current controller code defines actions without the gripper being closed (the final value in the action list is 0.0). To match the feedback suggesting closing the gripper, the last value of the action list returned by the controller should be changed from 0.0 to 1.0, to represent the action of closing the gripper.\",\n",
      " \"suggestion\": {\n",
      "   \"__code7\": \"def controller(obs: str):\\n    '''\\n    The controller takes in an observation and returns an action.\\n    '''\\n    return [-0.07, 0.68, 0.05, 1.0]\"\n",
      " }\n",
      "}\n",
      "Time Step: 24 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 0.0] at observation {\"hand_pos\": \"[-0.07  0.68  0.12]\", \"gripper_distance_apart\": \"0.998\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.345e-04 -2.555e-04  1.488e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.999e-02  6.799e-01  1.200e-01  9.980e-01 -6.378e-02  6.788e-01\\n  1.942e-02  1.417e-04 -3.371e-04  1.890e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07  0.68  0.05]\", \"gripper_distance_apart\": \"0.999\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 4.750e-04  3.982e-05 -1.164e-05  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.998e-02  6.800e-01  5.005e-02  9.986e-01 -6.377e-02  6.788e-01\\n  1.941e-02  3.106e-04  3.451e-05 -1.162e-05  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The reward is 0.11753955168698375. Press on! You've moving in the right direction.But, consider closing the gripper. Consider closing the gripper. I suggest you move to pose [-0.07  0.68  0.05  1.  ]..\n",
      "Variable:\n",
      " def controller(obs: str):\n",
      "    '''\n",
      "    The controller takes in an observation and returns an action.\n",
      "    '''\n",
      "    return [-0.07, 0.68, 0.05, 1.0]\n",
      "Termination: False\n",
      "Truncation: True\n",
      "Sum of rewards: 2.4615617438451385\n",
      "Success: False\n",
      "# of optimization iterations: 3\n",
      "# of time steps: 32\n"
     ]
    }
   ],
   "source": [
    "# Create an optimizer (Delay=7)\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "action_space = env.env.action_space\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs: str):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "max_iter = None\n",
    "delay = 7\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env,\n",
    "    horizon=horizon,\n",
    "    controller=controller,\n",
    "    user_feedback=user_feedback,\n",
    "    optimizer=optimizer,\n",
    "    max_iter=max_iter,\n",
    "    delay=delay,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Code as Policy for Metaworld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llfbench\n",
    "import autogen.trace as trace\n",
    "from autogen.trace.optimizers import FunctionOptimizer\n",
    "from llfbench.agents.utils import set_seed\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "# Config\n",
    "horizon = 20\n",
    "seed = 0\n",
    "env_name = \"llf-metaworld-pick-place-v2\"\n",
    "\n",
    "set_seed(seed)\n",
    "env = llfbench.make(env_name)\n",
    "env.seed(seed)\n",
    "\n",
    "\n",
    "@trace.trace_op()\n",
    "def reset(n_outputs=2):\n",
    "    \"\"\"\n",
    "    Reset the environment and return the initial observation and info.\n",
    "    \"\"\"\n",
    "    return env.reset()  # obs, info\n",
    "\n",
    "\n",
    "@trace.trace_op(n_outputs=5)\n",
    "def step(action):\n",
    "    \"\"\"\n",
    "    Take action in the environment and return the next observation, reward, done, and info.\n",
    "    \"\"\"\n",
    "    return env.step(action)  # next_obs, reward, termination, truncation, info\n",
    "\n",
    "\n",
    "def user_feedback(obs, action, next_obs):\n",
    "    \"\"\"\n",
    "    Provide feedback to the user.\n",
    "    \"\"\"\n",
    "    return f\"Taking action {action.data} at observation {obs['observation'].data} resulted in next observation {next_obs['observation'].data}. Recieved feedback {next_obs['feedback'].data}.\"\n",
    "\n",
    "\n",
    "# ### Optimization for single step\n",
    "# def single_step():\n",
    "#     optimizer = trace.optimizers.FunctionOptimizer(controller.parameters())\n",
    "\n",
    "#     obs, info = reset()\n",
    "#     optimizer.objective = f\"{optimizer.default_objective} Hint: {obs['instruction']}\"\n",
    "\n",
    "#     sum_of_rewards = 0\n",
    "#     for _ in range(horizon):\n",
    "#         try:\n",
    "#             action = controller(\n",
    "#                 obs[\"observation\"].detach()\n",
    "#             )  # Need a new node; otherwise, it would be back-propagated across time.\n",
    "#             next_obs, reward, termination, truncation, info = step(action)\n",
    "#             feedback = user_feedback(obs, action, next_obs)  # not traced\n",
    "#             obs = next_obs\n",
    "#             target = obs[\"observation\"]\n",
    "#         except trace.TraceExecutionError as e:\n",
    "#             feedback = str(e)\n",
    "#             target = e.exception_node\n",
    "\n",
    "#         # Optimization step\n",
    "#         optimizer.zero_feedback()\n",
    "#         optimizer.backward(target, feedback)  # obs = next obs\n",
    "#         optimizer.step(verbose=True)\n",
    "\n",
    "#         sum_of_rewards = reward + sum_of_rewards\n",
    "#         if termination or truncation:\n",
    "#             break\n",
    "\n",
    "#     return optimizer, sum_of_rewards\n",
    "\n",
    "\n",
    "def rollout(obs, horizon, controller):\n",
    "    # Reset the env outside\n",
    "    # Rollout for horizon steps\n",
    "\n",
    "    buffer = defaultdict(list)\n",
    "    for _ in range(horizon):\n",
    "        action = controller(obs[\"observation\"])\n",
    "        next_obs, reward, termination, truncation, info = step(action)\n",
    "        feedback = user_feedback(obs, action, next_obs)  # not traced\n",
    "        obs = next_obs\n",
    "        buffer[\"observation\"].append(obs)\n",
    "        buffer[\"action\"].append(action)\n",
    "        buffer[\"reward\"].append(reward)\n",
    "        buffer[\"termination\"].append(termination)\n",
    "        buffer[\"truncation\"].append(truncation)\n",
    "        buffer[\"info\"].append(info)\n",
    "        buffer[\"feedback\"].append(feedback)\n",
    "        done = termination or truncation\n",
    "        if done:\n",
    "            break\n",
    "    return buffer, done\n",
    "\n",
    "\n",
    "### Optimization for multi step\n",
    "def multi_step(controller, n_rollouts=15, rollout_horizon=3, horizon=30):\n",
    "    optimizer = trace.optimizers.FunctionOptimizer(controller.parameters())\n",
    "    checkpoints = defaultdict(list)\n",
    "    data = list()\n",
    "    traj = defaultdict(list)\n",
    "    done = True\n",
    "    while len(data) < n_rollouts:  # iterations\n",
    "        error = None\n",
    "        try:  # Trace the rollout; detach init_obs to avoid back-propagating across time.\n",
    "            if (len(traj[\"action\"]) % horizon == 0) or done:\n",
    "                traj = defaultdict(list)\n",
    "                data.append(traj)\n",
    "                init_obs, info = reset()\n",
    "                # not traced\n",
    "                instruction = init_obs[\"instruction\"].data\n",
    "                hint = (\n",
    "                    instruction\n",
    "                    + \"The controller should be a function that depends on the observation, not just outputing a constant action.\"\n",
    "                )\n",
    "                optimizer.objective = f\"{optimizer.default_objective} Hint: {hint}\"\n",
    "            buffer, done = rollout(init_obs.detach(), rollout_horizon, controller)\n",
    "\n",
    "        except trace.TraceExecutionError as e:\n",
    "            error = e\n",
    "\n",
    "        if error is None:\n",
    "            feedback = \"\\n\".join(buffer[\"feedback\"])\n",
    "            target = buffer[\"observation\"][-1][\"observation\"]  # last observation\n",
    "        else:\n",
    "            feedback = str(error)\n",
    "            target = error.exception_node\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(target, feedback)  # obs = next obs\n",
    "        optimizer.step(verbose=True)\n",
    "\n",
    "        # Log\n",
    "        if error is None:\n",
    "            for key in buffer:  # Update log data\n",
    "                traj[key].extend([d.data if isinstance(d, trace.Node) else d for d in buffer[key]])\n",
    "\n",
    "            print(f\"Sum of rewards so far: {sum([r for r in traj['reward']])}\")\n",
    "        print(\"Parameters:\")\n",
    "        for p in optimizer.parameters:\n",
    "            print(p.data)\n",
    "\n",
    "        checkpoints[\"variables\"].append(copy.deepcopy(controller))\n",
    "\n",
    "    return optimizer, checkpoints, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need ablation of not tracing step and reset\n",
    "# Need ablation of ignoring some info in propagated feedback\n",
    "# Need to test backward across time.\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return env.action_space.sample()\n",
    "\n",
    "\n",
    "optimizer, checkpoints, data = multi_step(controller, n_rollouts=15, rollout_horizon=5, horizon=30)\n",
    "results = {\"checkpoints\": checkpoints, \"data\": data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for traj in data:\n",
    "    print(f\"Sum of rewards: {sum(traj['reward'])}\")\n",
    "\n",
    "\n",
    "with open(\"results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

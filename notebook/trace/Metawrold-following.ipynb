{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Code as Policy for Metaworld\n",
    "\n",
    "This notebook shows a basic example of using the optimizer to follow language feedback online during interaction. We decorate the gym env to make it traceable and then run the optimizer for every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "seed = 0\n",
    "horizon = 30\n",
    "env_name = \"llf-metaworld-pick-place-v2\"\n",
    "stepsize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llfbench\n",
    "import autogen.trace as trace\n",
    "from autogen.trace.optimizers import FunctionOptimizer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TracedEnv:\n",
    "\n",
    "    def __init__(self, env_name, seed=0):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.env = llfbench.make(env_name)\n",
    "        self.env.reset(seed=seed)\n",
    "        self.env.action_space.seed(seed)\n",
    "\n",
    "    @trace.trace_op(n_outputs=2)\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial observation and info.\n",
    "        \"\"\"\n",
    "        return self.env.reset()  # obs, info\n",
    "\n",
    "    @trace.trace_op(n_outputs=5)\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action in the environment and return the next observation, reward, termination, truncation, and info.\n",
    "        \"\"\"\n",
    "        return self.env.step(action)\n",
    "\n",
    "\n",
    "def user_feedback(obs, action, next_obs):\n",
    "    \"\"\"\n",
    "    Provide feedback from the user.\n",
    "    \"\"\"\n",
    "    return f\"Taking action {action.data} at observation {obs['observation'].data} resulted in next observation {next_obs['observation'].data}. Recieved feedback {next_obs['feedback'].data}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we run the expert policy for the environment to get a sense of oracle performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space maximum and minimum values are equal. Actual equal coordinates: [(36,), (37,), (38,)]\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/metaworld/policies/policy.py:42: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.expert_action to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.expert_action` for environment variables or `env.get_wrapper_attr('expert_action')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards: 16.13824992438523\n",
      "Success: True\n",
      "Termination: False\n",
      "Truncation: False\n",
      "# of time steps: 7\n"
     ]
    }
   ],
   "source": [
    "def expert_run(env, horizon):\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Rollout\n",
    "    sum_of_rewards = 0\n",
    "    t = 0\n",
    "    expert_action = None\n",
    "    while t < horizon:\n",
    "        action = env.env.action_space.sample() if expert_action is None else expert_action\n",
    "        next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        expert_action = env.env.expert_action\n",
    "\n",
    "        sum_of_rewards += reward.data  # not traced\n",
    "        t += 1\n",
    "        if termination or truncation or info.data[\"success\"]:\n",
    "            break\n",
    "\n",
    "    print(\"Sum of rewards:\", sum_of_rewards)\n",
    "    print(\"Success:\", info.data[\"success\"])\n",
    "    print(\"Termination:\", termination.data)\n",
    "    print(\"Truncation:\", truncation.data)\n",
    "    print(\"# of time steps:\", t)\n",
    "\n",
    "    return sum_of_rewards, info.data[\"success\"], termination.data, truncation.data\n",
    "\n",
    "\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "sum_of_rewards, success, termination, truncation = expert_run(env, horizon=horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the optimizer is using code as actions, to follow the languge feedback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Based on the feedback, the action taken as a result of the current controller's logic resulted in an undesired state transition and the suggestion indicates a specific action vector [-0.07, 0.68, 0.12, 0.0] that should be taken instead. Since the feedback step size is 1, we fully trust the feedback and directly incorporate the recommended action into the controller logic. Therefore, the controller function needs to be adjusted to return this specific action vector, rather than using action_space.sample().\",\n",
      "\"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjusted to provide specific action based on feedback suggesting a more efficient move.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.12, 0.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 1 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [ 0.27392337 -0.46042657 -0.91805295 -0.96694473] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[0.165 0.45  0.066]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 5.915e-05 -3.631e-04  1.886e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 1.543e-01  4.586e-01  6.771e-02  1.000e+00 -6.375e-02  6.789e-01\\n  1.942e-02 -3.068e-04  6.263e-04  1.676e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The reward you just earned is 0.008956819426791872. The direction of the arm's latest move is wrong. Achieving the goal will now take longer than before. You should go to [-0.07  0.68  0.12  0.  ]..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjusted to provide specific action based on feedback suggesting a more efficient move.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.12, 0.0]\n",
      "LLM response:\n",
      " {\"reasoning\": \"The current code feedback indicates that the action taken, [-0.07, 0.68, 0.12, 0.0], was promising and resulted in positive feedback moving towards the goal. Since the feedback positively acknowledges the action and no specific direction or correction is provided to adjust this action, it suggests that no adjustments are necessary to the values within the controller function. Therefore, based on the stepsize of 1, which implies full trust in the feedback, there is no need to modify the given action values to improve the output.\",\n",
      " \"suggestion\": {}\n",
      "}\n",
      "LLM returns invalid format, cannot extract suggestions from JSON\n",
      "{\"reasoning\": \"The current code feedback indicates that the action taken, [-0.07, 0.68, 0.12, 0.0], was promising and resulted in positive feedback moving towards the goal. Since the feedback positively acknowledges the action and no specific direction or correction is provided to adjust this action, it suggests that no adjustments are necessary to the values within the controller function. Therefore, based on the stepsize of 1, which implies full trust in the feedback, there is no need to modify the given action values to improve the output.\",\n",
      " \"suggestion\": {}\n",
      "}\n",
      "Time Step: 2 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[0.165 0.45  0.066]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 5.915e-05 -3.631e-04  1.886e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 1.543e-01  4.586e-01  6.771e-02  1.000e+00 -6.375e-02  6.789e-01\\n  1.942e-02 -3.068e-04  6.263e-04  1.676e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[0.038 0.558 0.105]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.055e-04  1.445e-04  1.938e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.757e-02  5.489e-01  1.030e-01  9.972e-01 -6.380e-02  6.788e-01\\n  1.941e-02  2.736e-04 -8.266e-04  2.046e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You've received a reward of 0.0036948412547348182. You are making progress towards achieving the goal. Keep it up! Action [-0.07  0.68  0.12  0.  ] is promising..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjusted to provide specific action based on feedback suggesting a more efficient move.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.12, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given the feedback, it indicates that adjusting the third value (previously 0.12) to 0.05 in the action array produced by the controller function could potentially bring better results towards achieving the goal. Since the stepsize is 1, we completely trust this feedback and make the suggested modification. Changing this value seems to align with the guidance received ('Moving to [-0.07  0.68  0.05  0.  ] now is a good idea.'). This adjustment should ideally lead to obtaining a better outcome because it is indicated as a good move based on the environmental feedback.\",\n",
      "\"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjusted to provide specific action based on feedback suggesting a more efficient move.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.68, 0.05, 0.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 3 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.07, 0.68, 0.12, 0.0] at observation {\"hand_pos\": \"[0.038 0.558 0.105]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.055e-04  1.445e-04  1.938e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.757e-02  5.489e-01  1.030e-01  9.972e-01 -6.380e-02  6.788e-01\\n  1.941e-02  2.736e-04 -8.266e-04  2.046e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.068  0.68   0.126]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.760e-04 -3.164e-04  1.882e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.666e-02  6.775e-01  1.268e-01  9.968e-01 -6.375e-02  6.789e-01\\n  1.941e-02 -3.540e-04  6.338e-04  1.886e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The previous step's reward was 0.08848663941327087. You're closing in on your goal. Keep at it! Moving to [-0.07  0.68  0.05  0.  ] now is a good idea..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjusted to provide specific action based on feedback suggesting a more efficient move.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.68, 0.05, 0.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given the feedback to consider closing the gripper and the recommended move to [-0.07, 0.69, 0.05, 1.0], it is clear that the last element of the action array in the controller function needs to be changed from 0.0 to 1.0. This change aligns with the feedback to adjust the function for a more efficient action by considering the gripper's state. The steps size of 1 indicates full trust in the feedback, suggesting the necessity of this update for the expected outcome.\",\n",
      "\"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjusted to provide specific action based on feedback suggesting a more efficient move.\\n    \\\"\\\"\\\"\\n    return [-0.07, 0.69, 0.05, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 4 of 30\n",
      "Iteration: 4\n",
      "Feedback: Taking action [-0.07, 0.68, 0.05, 0.0] at observation {\"hand_pos\": \"[-0.068  0.68   0.126]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.760e-04 -3.164e-04  1.882e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.666e-02  6.775e-01  1.268e-01  9.968e-01 -6.375e-02  6.789e-01\\n  1.941e-02 -3.540e-04  6.338e-04  1.886e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.069  0.688  0.051]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.591e-04  2.772e-04  1.658e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.865e-02  6.890e-01  5.311e-02  9.971e-01 -6.379e-02  6.788e-01\\n  1.941e-02  4.669e-04 -4.854e-04  1.631e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback 0.03777250788365562 points for you. Your progress is great! Continue down the same path.But, consider closing the gripper. Consider closing the gripper. I recommend that you move to [-0.07  0.69  0.05  1.  ]..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjusted to provide specific action based on feedback suggesting a more efficient move.\n",
      "    \"\"\"\n",
      "    return [-0.07, 0.69, 0.05, 1.0]\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Given the feedback indicating that the action [-0.07, 0.69, 0.05, 1.0] led to an outcome that veers off from the efficient path towards the goal, and the explicit suggestion to adjust the movement direction towards pose [0.04, 0.88, 0.24, 1.0], it's clear that we need to adjust the action output of the 'controller' function defined in __code0 to align with the suggested optimal pose. Since the stepsize is 1, indicating full trust in the feedback, we follow the suggestion exactly to improve performance.\",\n",
      " \"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    \\\"\\\"\\\"\\n    The controller takes in an observation and returns an action.\\n    Adjusted to provide specific action based on feedback suggesting a more efficient move.\\n    \\\"\\\"\\\"\\n    return [0.04, 0.88, 0.24, 1.0]\"\n",
      "}\n",
      "}\n",
      "Time Step: 5 of 30\n",
      "Iteration: 5\n",
      "Feedback: Taking action [-0.07, 0.69, 0.05, 1.0] at observation {\"hand_pos\": \"[-0.069  0.688  0.051]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.591e-04  2.772e-04  1.658e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.865e-02  6.890e-01  5.311e-02  9.971e-01 -6.379e-02  6.788e-01\\n  1.941e-02  4.669e-04 -4.854e-04  1.631e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07   0.689  0.049]\", \"gripper_distance_apart\": \"0.480\", \"puck_pos\": \"[-0.064  0.689  0.022]\", \"puck_rot\": \"[ 9.749e-05  3.289e-04 -1.037e-04  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.058e-02  6.891e-01  4.872e-02  4.814e-01 -6.429e-02  6.889e-01\\n  2.186e-02  1.698e-03  3.866e-04 -6.688e-04  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback 0.12181899154228519 points for you. Your move direction is off. Getting to your objective will now take longer. My suggestion is that you go towards pose [0.04 0.88 0.24 1.  ]..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjusted to provide specific action based on feedback suggesting a more efficient move.\n",
      "    \"\"\"\n",
      "    return [0.04, 0.88, 0.24, 1.0]\n",
      "LLM response:\n",
      " {\"reasoning\": \"Given the feedback, it seems the action taken based on the initial observation was heading in the right direction, as evidenced by the positive feedback and a specific reward. The goal pose was [0.04, 0.88, 0.24, 1.0], which aligns with the action returned from the controller. There's no direct feedback indicating that the current action/script led to an incorrect or inefficient result. Instead, the feedback suggests continuity in the direction of the provided action. No variables need changing based on the feedback and the stepsize, as the feedback confirms the effectiveness of the current approach, with the stepsize implying full trust in this feedback. The code's intention is to dynamically generate actions based on observations with the goal of reaching a certain pose, and the feedback confirms success in progressing towards this aim. Therefore, no changes are suggested.\",\n",
      " \"suggestion\": {}\n",
      "}\n",
      "LLM returns invalid format, cannot extract suggestions from JSON\n",
      "{\"reasoning\": \"Given the feedback, it seems the action taken based on the initial observation was heading in the right direction, as evidenced by the positive feedback and a specific reward. The goal pose was [0.04, 0.88, 0.24, 1.0], which aligns with the action returned from the controller. There's no direct feedback indicating that the current action/script led to an incorrect or inefficient result. Instead, the feedback suggests continuity in the direction of the provided action. No variables need changing based on the feedback and the stepsize, as the feedback confirms the effectiveness of the current approach, with the stepsize implying full trust in this feedback. The code's intention is to dynamically generate actions based on observations with the goal of reaching a certain pose, and the feedback confirms success in progressing towards this aim. Therefore, no changes are suggested.\",\n",
      " \"suggestion\": {}\n",
      "}\n",
      "Time Step: 6 of 30\n",
      "Iteration: 6\n",
      "Feedback: Taking action [0.04, 0.88, 0.24, 1.0] at observation {\"hand_pos\": \"[-0.07   0.689  0.049]\", \"gripper_distance_apart\": \"0.480\", \"puck_pos\": \"[-0.064  0.689  0.022]\", \"puck_rot\": \"[ 9.749e-05  3.289e-04 -1.037e-04  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.058e-02  6.891e-01  4.872e-02  4.814e-01 -6.429e-02  6.889e-01\\n  2.186e-02  1.698e-03  3.866e-04 -6.688e-04  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[0.032 0.815 0.21 ]\", \"gripper_distance_apart\": \"0.446\", \"puck_pos\": \"[0.035 0.817 0.181]\", \"puck_rot\": \"[0.028 0.008 0.002 1.   ]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[0.031 0.81  0.203 0.446 0.034 0.812 0.175 0.029 0.008 0.007 1.    0.\\n 0.    0.    0.    0.    0.    0.   ]\"}. Recieved feedback Your instantaneous reward is 5.8812023482863225. You're heading the right way. Go on! Attempt to reach pose [0.04 0.88 0.24 1.  ] next..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    \"\"\"\n",
      "    The controller takes in an observation and returns an action.\n",
      "    Adjusted to provide specific action based on feedback suggesting a more efficient move.\n",
      "    \"\"\"\n",
      "    return [0.04, 0.88, 0.24, 1.0]\n",
      "Sum of rewards: 16.14193214780706\n",
      "Success: True\n",
      "Termination: False\n",
      "Truncation: False\n",
      "# of optimization iterations: 6\n",
      "# of time steps: 7\n"
     ]
    }
   ],
   "source": [
    "def single_step(env, horizon, user_feedback, controller, optimizer, max_iter=None):\n",
    "    \"\"\"Run optimizer step for every time step.\"\"\"\n",
    "\n",
    "    max_iter = max_iter or horizon * 2\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Rollout\n",
    "    sum_of_rewards = 0\n",
    "    t = 0\n",
    "    i = 0\n",
    "    while t < horizon and i < max_iter:\n",
    "        error = None\n",
    "        try:\n",
    "            # Detach; otherwise, it would be back-propagated across time.\n",
    "            action = controller(obs[\"observation\"].detach())\n",
    "            next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        except trace.TraceExecutionError as e:\n",
    "            error = e\n",
    "\n",
    "        if error is None:\n",
    "            feedback = user_feedback(obs, action, next_obs)  # not traced\n",
    "            obs = next_obs\n",
    "            target = next_obs[\"observation\"]\n",
    "            # Log\n",
    "            sum_of_rewards += reward.data  # not traced\n",
    "            t += 1  # time step\n",
    "            if termination or truncation or info.data[\"success\"]:\n",
    "                break\n",
    "        else:  # Self debugging\n",
    "            feedback = str(error)\n",
    "            target = error.exception_node\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(target, feedback)  # obs = next obs\n",
    "        optimizer.step(verbose=\"output\")\n",
    "        i += 1  # optimization iteration\n",
    "\n",
    "        print(f\"Time Step: {t} of {horizon}\")\n",
    "        print(f\"Iteration: {i}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        print(f\"Variable:\\n {controller.parameter.data}\")\n",
    "\n",
    "    print(\"Sum of rewards:\", sum_of_rewards)\n",
    "    print(\"Success:\", info.data[\"success\"])\n",
    "    print(\"Termination:\", termination.data)\n",
    "    print(\"Truncation:\", truncation.data)\n",
    "    print(\"# of optimization iterations:\", i)\n",
    "    print(\"# of time steps:\", t)\n",
    "\n",
    "    return sum_of_rewards, info.data[\"success\"], termination.data, truncation.data, optimizer\n",
    "\n",
    "\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "\n",
    "\n",
    "action_space = env.env.action_space\n",
    "\n",
    "# Declare the controller to be trainable\n",
    "\n",
    "\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env, horizon=horizon, controller=controller, user_feedback=user_feedback, optimizer=optimizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Code as Policy for Metaworld\n",
    "\n",
    "This notebook shows a basic example of using the optimizer to follow language feedback online during interaction. We decorate the gym env to make it traceable and then run the optimizer for every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "\n",
    "seed = 0\n",
    "horizon = 30\n",
    "env_name = \"llf-metaworld-pick-place-v2\"\n",
    "stepsize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llfbench\n",
    "import autogen.trace as trace\n",
    "from autogen.trace.optimizers import FunctionOptimizer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TracedEnv:\n",
    "\n",
    "    def __init__(self, env_name, seed=0):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        self.env = llfbench.make(env_name)\n",
    "        self.env.reset(seed=seed)\n",
    "        self.env.action_space.seed(seed)\n",
    "\n",
    "    @trace.trace_op()\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial observation and info.\n",
    "        \"\"\"\n",
    "        return self.env.reset()  # obs, info\n",
    "\n",
    "    @trace.trace_op()\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action in the environment and return the next observation, reward, termination, truncation, and info.\n",
    "        \"\"\"\n",
    "        return self.env.step(action)\n",
    "\n",
    "\n",
    "def user_feedback(obs, action, next_obs):\n",
    "    \"\"\"\n",
    "    Provide feedback from the user.\n",
    "    \"\"\"\n",
    "    return f\"Taking action {action.data} at observation {obs['observation'].data} resulted in next observation {next_obs['observation'].data}. Recieved feedback {next_obs['feedback'].data}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:32: UserWarning: \u001b[33mWARN: A Box observation space maximum and minimum values are equal. Actual equal coordinates: [(36,), (37,), (38,)]\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/spaces/box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/metaworld/policies/policy.py:42: UserWarning: Constant(s) may be too high. Environments clip response to [-1, 1]\n",
      "  warnings.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/utils/passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'str'>\u001b[0m\n",
      "  logger.warn(\n",
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.expert_action to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.expert_action` for environment variables or `env.get_wrapper_attr('expert_action')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards: 16.13824992438523\n",
      "Success: True\n",
      "Termination: False\n",
      "Truncation: False\n",
      "# of time steps: 7\n"
     ]
    }
   ],
   "source": [
    "def expert_run(env, horizon):\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Rollout\n",
    "    sum_of_rewards = 0\n",
    "    t = 0\n",
    "    expert_action = None\n",
    "    while t < horizon:\n",
    "        action = env.env.action_space.sample() if expert_action is None else expert_action\n",
    "        next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        expert_action = env.env.expert_action\n",
    "\n",
    "        sum_of_rewards += reward.data  # not traced\n",
    "        t += 1\n",
    "        if termination or truncation or info.data[\"success\"]:\n",
    "            break\n",
    "\n",
    "    print(\"Sum of rewards:\", sum_of_rewards)\n",
    "    print(\"Success:\", info.data[\"success\"])\n",
    "    print(\"Termination:\", termination.data)\n",
    "    print(\"Truncation:\", truncation.data)\n",
    "    print(\"# of time steps:\", t)\n",
    "\n",
    "    return sum_of_rewards, info.data[\"success\"], termination.data, truncation.data\n",
    "\n",
    "\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "sum_of_rewards, success, termination, truncation = expert_run(env, horizon=horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Based on the feedback, the action taken led to an undesired state and direction, as indicated by the provided coordinates to move towards. Since our controller's action is generated by action_space.sample(), and we have a full trust in the feedback (stepsize of 1), changing the controller to generate more suitable actions towards the recommended coordinates [-0.07  0.68  0.12  0.] is required. The __code0, which defines the behavior of the controller, should be updated to reflect this desired action direction to improve the performance as per the feedback.\",\n",
      "\"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    return np.array([-0.07, 0.68, 0.12, 0.])\"\n",
      "}\n",
      "}\n",
      "Time Step: 1 of 30\n",
      "Iteration: 1\n",
      "Feedback: Taking action [ 0.27392337 -0.46042657 -0.91805295 -0.96694473] at observation {\"hand_pos\": \"[0.006 0.6   0.194]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.02 ]\", \"puck_rot\": \"[0. 0. 0. 1.]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 0.006  0.6    0.194  1.    -0.064  0.679  0.02   0.     0.     0.\\n  1.     0.     0.     0.     0.     0.     0.     0.   ]\"} resulted in next observation {\"hand_pos\": \"[0.165 0.45  0.066]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 5.915e-05 -3.631e-04  1.886e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 1.543e-01  4.586e-01  6.771e-02  1.000e+00 -6.375e-02  6.789e-01\\n  1.942e-02 -3.068e-04  6.263e-04  1.676e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The reward you just earned is 0.008956819426791872. The direction of the arm's latest move is wrong. Achieving the goal will now take longer than before. You should go to [-0.07  0.68  0.12  0.  ]..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    return np.array([-0.07, 0.68, 0.12, 0.])\n",
      "LLM response:\n",
      " {\"reasoning\": \"The feedback indicates that the action taken with the controller's current settings resulted in a positive reward and progress towards the goal, which means the controller's logic doesn't need to be changed based on the feedback provided. The stepsize value of 1 suggests we completely trust this feedback. However, since the feedback is positive about making progress and considers the action promising, there isn't a clear directive or necessity to adjust the values the controller function generates. It implies that the current action [-0.07  0.68, 0.12, 0.] is effective under the circumstances presented by the given initial observation. Since the feedback does not highlight a specific issue or suggest an improvement, no changes to the variable values (specifically, the controller's code which defines action output) are warranted. All actions are based on the eval function which evaluated the controller correctly, leading to a positive outcome as per feedback.\", \"suggestion\": {}}\n",
      "\n",
      "\n",
      "Cannot extract suggestion from LLM's response: \n",
      "{\"reasoning\": \"The feedback indicates that the action taken with the controller's current settings resulted in a positive reward and progress towards the goal, which means the controller's logic doesn't need to be changed based on the feedback provided. The stepsize value of 1 suggests we completely trust this feedback. However, since the feedback is positive about making progress and considers the action promising, there isn't a clear directive or necessity to adjust the values the controller function generates. It implies that the current action [-0.07  0.68, 0.12, 0.] is effective under the circumstances presented by the given initial observation. Since the feedback does not highlight a specific issue or suggest an improvement, no changes to the variable values (specifically, the controller's code which defines action output) are warranted. All actions are based on the eval function which evaluated the controller correctly, leading to a positive outcome as per feedback.\", \"suggestion\": {}}\n",
      "\n",
      "\n",
      "Time Step: 2 of 30\n",
      "Iteration: 2\n",
      "Feedback: Taking action [-0.07  0.68  0.12  0.  ] at observation {\"hand_pos\": \"[0.165 0.45  0.066]\", \"gripper_distance_apart\": \"1.000\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 5.915e-05 -3.631e-04  1.886e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 1.543e-01  4.586e-01  6.771e-02  1.000e+00 -6.375e-02  6.789e-01\\n  1.942e-02 -3.068e-04  6.263e-04  1.676e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[0.038 0.558 0.105]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.055e-04  1.445e-04  1.938e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.757e-02  5.489e-01  1.030e-01  9.972e-01 -6.380e-02  6.788e-01\\n  1.941e-02  2.736e-04 -8.266e-04  2.046e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback You've received a reward of 0.0036948412547348182. You are making progress towards achieving the goal. Keep it up! Action [-0.07  0.68  0.12  0.  ] is promising..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    return np.array([-0.07, 0.68, 0.12, 0.])\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Since the feedback suggests that moving to [-0.07  0.68  0.05  0.  ] now is a good idea, and the stepsize is 1 which means we completely trust the feedback, we should update the __code0 function to return np.array([-0.07, 0.68, 0.05, 0.]) instead of np.array([-0.07, 0.68, 0.12, 0.]) in order to align the action with the feedback.\",\n",
      " \"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    return np.array([-0.07, 0.68, 0.05, 0.])\"\n",
      "}\n",
      "}\n",
      "Time Step: 3 of 30\n",
      "Iteration: 3\n",
      "Feedback: Taking action [-0.07  0.68  0.12  0.  ] at observation {\"hand_pos\": \"[0.038 0.558 0.105]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-1.055e-04  1.445e-04  1.938e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[ 4.757e-02  5.489e-01  1.030e-01  9.972e-01 -6.380e-02  6.788e-01\\n  1.941e-02  2.736e-04 -8.266e-04  2.046e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.068  0.68   0.126]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.760e-04 -3.164e-04  1.882e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.666e-02  6.775e-01  1.268e-01  9.968e-01 -6.375e-02  6.789e-01\\n  1.941e-02 -3.540e-04  6.338e-04  1.886e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback The previous step's reward was 0.08848663941327087. You're closing in on your goal. Keep at it! Moving to [-0.07  0.68  0.05  0.  ] now is a good idea..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    return np.array([-0.07, 0.68, 0.05, 0.])\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback indicates the need to change the action taken by making use of the controller function. Specifically, it suggests 'closing the gripper' which corresponds to changing the last value of the action array from 0 to 1. This action array is returned by the function defined in __code0. The recommended new action is [-0.07  0.69  0.05  1.].\",\n",
      " \"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    return np.array([-0.07, 0.69, 0.05, 1.])\"\n",
      "}\n",
      "}\n",
      "Time Step: 4 of 30\n",
      "Iteration: 4\n",
      "Feedback: Taking action [-0.07  0.68  0.05  0.  ] at observation {\"hand_pos\": \"[-0.068  0.68   0.126]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[ 1.760e-04 -3.164e-04  1.882e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.666e-02  6.775e-01  1.268e-01  9.968e-01 -6.375e-02  6.789e-01\\n  1.941e-02 -3.540e-04  6.338e-04  1.886e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.069  0.688  0.051]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.591e-04  2.772e-04  1.658e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.865e-02  6.890e-01  5.311e-02  9.971e-01 -6.379e-02  6.788e-01\\n  1.941e-02  4.669e-04 -4.854e-04  1.631e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback 0.03777250788365562 points for you. Your progress is great! Continue down the same path.But, consider closing the gripper. Consider closing the gripper. I recommend that you move to [-0.07  0.69  0.05  1.  ]..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    return np.array([-0.07, 0.69, 0.05, 1.])\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"The feedback suggests that the action taken by the controller function ([ -0.07, 0.69, 0.05, 1.]) is not leading to the desired outcome. The suggestion implies that the controller should aim towards a pose [0.04, 0.88, 0.24, 1.], which indicates that the values returned by the __code0 function need to be adjusted to match this suggestion. Given the stepsize is 1, which implies full trust in the feedback, the controller function should be modified to return an array closer to the suggested pose to optimize the result.\",\n",
      " \"suggestion\": {\n",
      "    \"__code0\": \"def controller(obs):\\n    return np.array([0.04, 0.88, 0.24, 1.])\"\n",
      "}\n",
      "}\n",
      "Time Step: 5 of 30\n",
      "Iteration: 5\n",
      "Feedback: Taking action [-0.07  0.69  0.05  1.  ] at observation {\"hand_pos\": \"[-0.069  0.688  0.051]\", \"gripper_distance_apart\": \"0.997\", \"puck_pos\": \"[-0.064  0.679  0.019]\", \"puck_rot\": \"[-2.591e-04  2.772e-04  1.658e-06  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-6.865e-02  6.890e-01  5.311e-02  9.971e-01 -6.379e-02  6.788e-01\\n  1.941e-02  4.669e-04 -4.854e-04  1.631e-06  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[-0.07   0.689  0.049]\", \"gripper_distance_apart\": \"0.480\", \"puck_pos\": \"[-0.064  0.689  0.022]\", \"puck_rot\": \"[ 9.749e-05  3.289e-04 -1.037e-04  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.058e-02  6.891e-01  4.872e-02  4.814e-01 -6.429e-02  6.889e-01\\n  2.186e-02  1.698e-03  3.866e-04 -6.688e-04  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"}. Recieved feedback 0.12181899154228519 points for you. Your move direction is off. Getting to your objective will now take longer. My suggestion is that you go towards pose [0.04 0.88 0.24 1.  ]..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    return np.array([0.04, 0.88, 0.24, 1.])\n",
      "LLM response:\n",
      " \n",
      "{ \"reasoning\": \"Given the problem statement, feedback, and evaluation, there's no indication or requirement to change the code or the controller's output. The action formed by the controller ([0.04 0.88 0.24 1. ]) is exactly as desired based on the feedback provided, aiming to reach the pose [0.04 0.88 0.24 1. ]. The feedback suggests that the action taken was appropriate ('You're heading the right way'), and there's no request or suggestion to alter the controller's behavior or its output. The 'variables' section contained the controller's definition, which outputs the exact action that aligns with the feedback instructions. Therefore, no changes are necessary to improve the code outcome related to the feedback provided.\",\n",
      "  \"suggestion\": {}\n",
      "}\n",
      "\n",
      "  \n",
      "Cannot extract suggestion from LLM's response: \n",
      "\n",
      "{ \"reasoning\": \"Given the problem statement, feedback, and evaluation, there's no indication or requirement to change the code or the controller's output. The action formed by the controller ([0.04 0.88 0.24 1. ]) is exactly as desired based on the feedback provided, aiming to reach the pose [0.04 0.88 0.24 1. ]. The feedback suggests that the action taken was appropriate ('You're heading the right way'), and there's no request or suggestion to alter the controller's behavior or its output. The 'variables' section contained the controller's definition, which outputs the exact action that aligns with the feedback instructions. Therefore, no changes are necessary to improve the code outcome related to the feedback provided.\",\n",
      "  \"suggestion\": {}\n",
      "}\n",
      "\n",
      "  \n",
      "Time Step: 6 of 30\n",
      "Iteration: 6\n",
      "Feedback: Taking action [0.04 0.88 0.24 1.  ] at observation {\"hand_pos\": \"[-0.07   0.689  0.049]\", \"gripper_distance_apart\": \"0.480\", \"puck_pos\": \"[-0.064  0.689  0.022]\", \"puck_rot\": \"[ 9.749e-05  3.289e-04 -1.037e-04  1.000e+00]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[-7.058e-02  6.891e-01  4.872e-02  4.814e-01 -6.429e-02  6.889e-01\\n  2.186e-02  1.698e-03  3.866e-04 -6.688e-04  1.000e+00  0.000e+00\\n  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00]\"} resulted in next observation {\"hand_pos\": \"[0.032 0.815 0.21 ]\", \"gripper_distance_apart\": \"0.446\", \"puck_pos\": \"[0.035 0.817 0.181]\", \"puck_rot\": \"[0.028 0.008 0.002 1.   ]\", \"goal_pos\": \"[0.039 0.878 0.244]\", \"_prev_obs\": \"[0.031 0.81  0.203 0.446 0.034 0.812 0.175 0.029 0.008 0.007 1.    0.\\n 0.    0.    0.    0.    0.    0.   ]\"}. Recieved feedback Your instantaneous reward is 5.8812023482863225. You're heading the right way. Go on! Attempt to reach pose [0.04 0.88 0.24 1.  ] next..\n",
      "Variable:\n",
      " def controller(obs):\n",
      "    return np.array([0.04, 0.88, 0.24, 1.])\n",
      "Sum of rewards: 16.14193214780706\n",
      "Success: True\n",
      "Termination: False\n",
      "Truncation: False\n",
      "# of optimization iterations: 6\n",
      "# of time steps: 7\n"
     ]
    }
   ],
   "source": [
    "def single_step(env, horizon, user_feedback, controller, optimizer, max_iter=None):\n",
    "    \"\"\"Run optimizer step for every time step.\"\"\"\n",
    "\n",
    "    max_iter = max_iter or horizon * 2\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    # Rollout\n",
    "    sum_of_rewards = 0\n",
    "    t = 0\n",
    "    i = 0\n",
    "    while t < horizon and i < max_iter:\n",
    "        error = None\n",
    "        try:\n",
    "            # Detach; otherwise, it would be back-propagated across time.\n",
    "            action = controller(obs[\"observation\"].detach())\n",
    "            next_obs, reward, termination, truncation, info = env.step(action)\n",
    "        except trace.TraceExecutionError as e:\n",
    "            error = e\n",
    "\n",
    "        if error is None:\n",
    "            feedback = user_feedback(obs, action, next_obs)  # not traced\n",
    "            obs = next_obs\n",
    "            target = next_obs[\"observation\"]\n",
    "            # Log\n",
    "            sum_of_rewards += reward.data  # not traced\n",
    "            t += 1  # time step\n",
    "            if termination or truncation or info.data[\"success\"]:\n",
    "                break\n",
    "        else:  # Self debugging\n",
    "            feedback = str(error)\n",
    "            target = error.exception_node\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.zero_feedback()\n",
    "        optimizer.backward(target, feedback)  # obs = next obs\n",
    "        optimizer.step(verbose=\"output\")\n",
    "        i += 1  # optimization iteration\n",
    "\n",
    "        print(f\"Time Step: {t} of {horizon}\")\n",
    "        print(f\"Iteration: {i}\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        print(f\"Variable:\\n {controller.parameter.data}\")\n",
    "\n",
    "    print(\"Sum of rewards:\", sum_of_rewards)\n",
    "    print(\"Success:\", info.data[\"success\"])\n",
    "    print(\"Termination:\", termination.data)\n",
    "    print(\"Truncation:\", truncation.data)\n",
    "    print(\"# of optimization iterations:\", i)\n",
    "    print(\"# of time steps:\", t)\n",
    "\n",
    "    return sum_of_rewards, info.data[\"success\"], termination.data, truncation.data, optimizer\n",
    "\n",
    "\n",
    "env = TracedEnv(env_name, seed=seed)\n",
    "\n",
    "\n",
    "action_space = env.env.action_space\n",
    "\n",
    "# Declare the controller to be trainable\n",
    "@trace.trace_op(trainable=True)\n",
    "def controller(obs):\n",
    "    \"\"\"\n",
    "    The controller takes in an observation and returns an action.\n",
    "    \"\"\"\n",
    "    return action_space.sample()\n",
    "\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = trace.optimizers.FunctionOptimizer(controller.parameters(), stepsize=stepsize)\n",
    "sum_of_rewards, success, termination, truncation, optimizer = single_step(\n",
    "    env, horizon=horizon, controller=controller, user_feedback=user_feedback, optimizer=optimizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

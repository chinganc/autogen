{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen.trace import trace_op, node\n",
    "from autogen.trace.optimizers import FunctionOptimizer\n",
    "from autogen.trace.nodes import GRAPH\n",
    "\n",
    "\n",
    "def blackbox(x):\n",
    "    return -x * 2\n",
    "\n",
    "\n",
    "@trace_op()\n",
    "def bar(x):\n",
    "    \"This is a test function, which does negative scaling.\"\n",
    "    return blackbox(x)\n",
    "\n",
    "\n",
    "def foo(x):\n",
    "    y = x + 1\n",
    "    return x * y\n",
    "\n",
    "\n",
    "# foobar is a composition of custom function and built-in functions\n",
    "\n",
    "\n",
    "def foobar(x):\n",
    "    return foo(bar(x))\n",
    "\n",
    "\n",
    "def user(x):\n",
    "    if x < 50:\n",
    "        return \"The number needs to be larger.\"\n",
    "    else:\n",
    "        return \"Success.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Backpropagation \n",
    "\n",
    "We apply `FunctionOptimizer` to change the input to the function `foobar` such that the simulated user is satisfied. To this end, we backpropagated the user's language feedback about the output, through the graph that connects the input to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Optimization based Language Feedback\n",
    "\n",
    "Here we show a small example of how to apply `trace` to optimize python objects based on language feedback. Here we want to change the input to function `foobar` such that output is large enough. `foobar` is a function that is composed of `foo` based on built-in operators and `bar` which is a blackbox function, whose information is only given via the docstring.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"933pt\" height=\"416pt\"\n",
       " viewBox=\"0.00 0.00 933.18 415.81\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 411.8133)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-411.8133 929.1758,-411.8133 929.1758,4 -4,4\"/>\n",
       "<!-- bar0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>bar0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"647.2828\" cy=\"-259.3833\" rx=\"234.5191\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.2828\" y=\"-270.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">bar0</text>\n",
       "<text text-anchor=\"middle\" x=\"647.2828\" y=\"-255.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[bar] This is a test function, which does negative scaling..</text>\n",
       "<text text-anchor=\"middle\" x=\"647.2828\" y=\"-240.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2.0</text>\n",
       "</g>\n",
       "<!-- multiply0 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>multiply0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"549.2828\" cy=\"-37.4767\" rx=\"200.6368\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"549.2828\" y=\"-48.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">multiply0</text>\n",
       "<text text-anchor=\"middle\" x=\"549.2828\" y=\"-33.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[multiply] This is a multiply operator of x and y.</text>\n",
       "<text text-anchor=\"middle\" x=\"549.2828\" y=\"-18.7767\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6.0</text>\n",
       "</g>\n",
       "<!-- bar0&#45;&gt;multiply0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>bar0&#45;&gt;multiply0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M649.3666,-221.6292C649.5261,-190.3193 646.0268,-145.5023 628.2828,-110.9533 622.734,-100.1493 614.9276,-90.1044 606.3633,-81.1271\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"608.5876,-78.4035 599.024,-73.8441 603.6569,-83.3723 608.5876,-78.4035\"/>\n",
       "</g>\n",
       "<!-- add0 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>add0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"451.2828\" cy=\"-148.43\" rx=\"167.6687\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"451.2828\" y=\"-159.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">add0</text>\n",
       "<text text-anchor=\"middle\" x=\"451.2828\" y=\"-144.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[add] This is an add operator of x and y.</text>\n",
       "<text text-anchor=\"middle\" x=\"451.2828\" y=\"-129.73\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3.0</text>\n",
       "</g>\n",
       "<!-- bar0&#45;&gt;add0 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>bar0&#45;&gt;add0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M583.3763,-223.2066C563.7193,-212.079 541.9717,-199.7679 521.8922,-188.4012\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"523.5264,-185.3044 513.0998,-183.4239 520.0779,-191.3961 523.5264,-185.3044\"/>\n",
       "</g>\n",
       "<!-- add0&#45;&gt;multiply0 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>add0&#45;&gt;multiply0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M483.9154,-111.4841C492.1752,-102.1326 501.136,-91.9873 509.6983,-82.2933\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"512.4618,-84.4515 516.4586,-74.6395 507.2153,-79.8175 512.4618,-84.4515\"/>\n",
       "</g>\n",
       "<!-- int0 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>int0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"197.2828\" cy=\"-259.3833\" rx=\"197.0658\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"197.2828\" y=\"-270.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">int0</text>\n",
       "<text text-anchor=\"middle\" x=\"197.2828\" y=\"-255.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[Node] This is a node in a computational graph.</text>\n",
       "<text text-anchor=\"middle\" x=\"197.2828\" y=\"-240.6833\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1</text>\n",
       "</g>\n",
       "<!-- int0&#45;&gt;add0 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>int0&#45;&gt;add0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M276.2577,-224.8852C304.5103,-212.5437 336.4817,-198.5779 365.2973,-185.9905\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"366.7706,-189.1664 374.5333,-181.956 363.9684,-182.7517 366.7706,-189.1664\"/>\n",
       "</g>\n",
       "<!-- float0 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>float0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"647.2828\" cy=\"-370.3366\" rx=\"277.786\" ry=\"37.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.2828\" y=\"-381.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">float0</text>\n",
       "<text text-anchor=\"middle\" x=\"647.2828\" y=\"-366.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">[ParameterNode] This is a ParameterNode in a computational graph.</text>\n",
       "<text text-anchor=\"middle\" x=\"647.2828\" y=\"-351.6366\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">&#45;1.0</text>\n",
       "</g>\n",
       "<!-- float0&#45;&gt;bar0 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>float0&#45;&gt;bar0</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M647.2828,-332.7729C647.2828,-324.549 647.2828,-315.7367 647.2828,-307.1736\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"650.7829,-306.9083 647.2828,-296.9083 643.7829,-306.9084 650.7829,-306.9083\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7febff212c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-step optimization example\n",
    "x = node(-1.0, trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "output = foobar(x)\n",
    "feedback = user(output.data)\n",
    "optimizer.zero_feedback()\n",
    "optimizer.backward(output, feedback, visualize=True)  # this is equivalent to the below line\n",
    "# output.backward(feedback, propagator=optimizer.propagator, visualize=visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The propagated feedback contains graph structure, data of the nodes in the graph, and the transformation used in the graph. They're presented in a python-like syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Feedback\n",
      "Graph:\n",
      "  1: bar0 = bar(x=float0)\n",
      "  2: add0 = add(x=bar0, y=int0)\n",
      "  3: multiply0 = multiply(x=bar0, y=add0)\n",
      "Roots:\n",
      "  int0: (1, None)\n",
      "  float0: (-1.0, None)\n",
      "Others:\n",
      "  bar0: (2.0, None)\n",
      "  add0: (3.0, None)\n",
      "Documentation:\n",
      "  bar: [bar] This is a test function, which does negative scaling..\n",
      "  add: [add] This is an add operator of x and y.\n",
      "  multiply: [multiply] This is a multiply operator of x and y.\n",
      "Output:\n",
      "  multiply0: (6.0, None)\n",
      "User Feedback:\n",
      "  The number needs to be larger.\n"
     ]
    }
   ],
   "source": [
    "from autogen.trace.optimizers.function_optimizer import node_to_function_feedback\n",
    "\n",
    "print(\"Function Feedback\")\n",
    "for k, v in x.feedback.items():\n",
    "    v = v[0]\n",
    "    f_feedback = node_to_function_feedback(v)\n",
    "    print(\"Graph:\")\n",
    "    for kk, vv in f_feedback.graph:\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Roots:\")\n",
    "    for kk, vv in f_feedback.roots.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Others:\")\n",
    "    for kk, vv in f_feedback.others.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Documentation:\")\n",
    "    for kk, vv in f_feedback.documentation.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"Output:\")\n",
    "    for kk, vv in f_feedback.output.items():\n",
    "        print(f\"  {kk}: {vv}\")\n",
    "    print(\"User Feedback:\")\n",
    "    print(f\"  {f_feedback.user_feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the feedback is propagated, we can call the optimization to change the variable based on the feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt\n",
      " \n",
      "You're tasked debug and solve a coding/algorithm problem. You will see the code, the documentation of each function used in the code, and the feedback about the code's execution result.\n",
      "\n",
      "Specifically, a problem will be composed of the following parts:\n",
      "- #Code: the code whose results you need to improve.\n",
      "- #Documentation: the documentation of each function used in the code.\n",
      "- #Variables: the values of the variables that you need to change.\n",
      "- #Inputs: the values of other inputs to the code\n",
      "- #Others: the intermediate values created through the code execution.\n",
      "- #Outputs: the result of the code.\n",
      "- #Feedback: the feedback about the code's execution result.\n",
      "\n",
      "In #Variables, #Outputs, and #Others, the format is:\n",
      "<type> <variable_name> = <value>\n",
      "You need to change the <value> of the variables in #Variables to improve the code's output in accordance to #Feedback and their data types specified in <type>. If <type> is (code), it means <value> is the source code of a python code, which may include docstring and definitions.\n",
      "The explanation in #Documentation might be incomplete and just contain high-level description of each function. You can use the values in #Others to help infer how those functions work.\n",
      "\n",
      "Objective: Your goal is to improve the code's output based on the feedback by changing variables used in the code.\n",
      "\n",
      "Output format:\n",
      "\n",
      "You should write down your thought process and finally make a suggestion of the desired values of #Variables. You cannot change the lines of code in #Code but only the values in #Variables. When <type> of a variable is (code), you should write the new definition in the format of python code without syntax errors.\n",
      "Your output should be in the following json format, satisfying the json syntax (json does support single quotes):\n",
      "{\n",
      "\"reasoning\": <Your reasoning>,\n",
      "\"suggestion\": {\n",
      "    <variable_1>: <suggested_value_1>,\n",
      "    <variable_2>: <suggested_value_2>,\n",
      "}\n",
      "}\n",
      "\n",
      "If no changes are needed, just output TERMINATE_UPDATE as opposed to the json format above.\n",
      "\n",
      "\n",
      "Here is an example of problem instance:\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "#Code\n",
      "y = add(x=a,y=b)\n",
      "z = subtract(x=y, y=c)\n",
      "\n",
      "#Documentation\n",
      "add: add x and y \n",
      "subtract: subtract y from x\n",
      "\n",
      "#Variables\n",
      "(int) a = 5\n",
      "\n",
      "#Inputs\n",
      "(int) b = 1\n",
      "(int) c = 5\n",
      "\n",
      "#Others\n",
      "(int) y = 6\n",
      "\n",
      "#Outputs\n",
      "(int) z = 1\n",
      "\n",
      "#Feedback:\n",
      "The result of the code is not as expected. The result should be 10, but the code returns 1\n",
      "\n",
      "\n",
      "================================\n",
      "\n",
      "Below is an ideal response for the problem above.\n",
      "\n",
      "================================\n",
      "\n",
      "\n",
      "{\"reasoning\": 'In this case, the desired response would be to change the value of input a to 14, as that would make the code return 10.',\n",
      " \"suggestion\": {\"a\": 10}\n",
      "}\n",
      "\n",
      "\n",
      "================================\n",
      "\n",
      "Now you see problem instance:\n",
      "\n",
      "\n",
      "#Code\n",
      "bar0 = bar(x=float0)\n",
      "add0 = add(x=bar0, y=int0)\n",
      "multiply0 = multiply(x=bar0, y=add0)\n",
      "\n",
      "#Documentation\n",
      "[bar] This is a test function, which does negative scaling..\n",
      "[add] This is an add operator of x and y.\n",
      "[multiply] This is a multiply operator of x and y.\n",
      "\n",
      "#Variables\n",
      "(float) float0=-1.0\n",
      "\n",
      "#Inputs\n",
      "(int) int0=1\n",
      "\n",
      "#Others\n",
      "(float) bar0=2.0\n",
      "(float) add0=3.0\n",
      "\n",
      "#Outputs\n",
      "(float) multiply0=6.0\n",
      "\n",
      "#Feedback:\n",
      "The number needs to be larger.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Here are some feasible but not optimal solutions (i.e. not causing errors but not necessarily achieving desired behaviors) for the problem instance above:\n",
      "\n",
      "Example 1:\n",
      "(float) float0=-1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LLM response:\n",
      " {\n",
      "\"reasoning\": \"Since the feedback indicates that the number needs to be larger and looking at the code and the result of each operation, changing 'float0' to a value that, after being processed by the 'bar' function (which does a negative scaling, indicated by the behavior of turning -1.0 into 2.0), results in a larger final product after addition and multiplication, could achieve the desired output. As 'multiply0=6.0' is not deemed large enough, increasing the base value before the 'bar' operation is a direct way to scale up the final result. Given the linear transformation seen from bar0, one way to increase the final number without knowing the exact implementation of 'bar' could be to start with a value less than -1.0, as the negative scaling appears to invert and possibly magnify the input value (considering -1 to 2 transformation). Trying a lesser value like -2.0 for 'float0' could lead to a larger outcome through these operations, hence adhering to the feedback.\",\n",
      " \"suggestion\": {\"float0\": -2.0}\n",
      "}\n",
      "\n",
      "After step\n",
      "old variable -1.0\n",
      "new variable -2.0\n"
     ]
    }
   ],
   "source": [
    "old_variable = x.data\n",
    "optimizer.step(verbose=True)\n",
    "\n",
    "print(\"\\nAfter step\")\n",
    "print(\"old variable\", old_variable)\n",
    "print(\"new variable\", x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Full Optimization Loop\n",
    "\n",
    "We can apply the steps above repetitively to create a training loop to optimize the variable according to the user. Notice because of the way `foobar` works, the optimizer actually needs to change the input to be lower in order to make the output to be larger (which is what the user suggests). \n",
    "\n",
    "This is a non-trivial problem, becasue the optimizer sees only\n",
    "\n",
    "```\n",
    "output = blackbox(x) * (blackbox(x)+1)\n",
    "```\n",
    "\n",
    "and the hint/docstring `\"This is a test function, which does scaling and negation.\"` about how `blackbox` works. The optimizer needs to figure out how to change the input based on this vague information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=-1.0, output=6.0, feedback=The number needs to be larger.\n",
      "variable=-2.0, output=20.0, feedback=The number needs to be larger.\n",
      "variable=-4.0, output=72.0, feedback=Success.\n",
      "History\n",
      "  0: -1.0\n",
      "  1: -2.0\n",
      "  2: -4.0\n",
      "  3: -4.0\n"
     ]
    }
   ],
   "source": [
    "# A small example of how to use the optimizer in a loop\n",
    "GRAPH.clear()\n",
    "x = node(-1.0, trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "history = [x.data]\n",
    "feedback = \"\"\n",
    "while feedback.lower() != \"Success.\".lower():\n",
    "    output = foobar(x)\n",
    "    feedback = user(output.data)\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(output, feedback)\n",
    "    print(f\"variable={x.data}, output={output.data}, feedback={feedback}\")  # logging\n",
    "    optimizer.step()\n",
    "    history.append(x.data)  # logging\n",
    "\n",
    "print(\"History\")\n",
    "for i, v in enumerate(history):\n",
    "    print(f\"  {i}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of optimizing strings\n",
    "\n",
    "Below is a similar example, except the variable is written in text and is converted by a poor converter to numbers before inputting to `foo` and `bar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trace_op()\n",
    "def convert_english_to_numbers(x):\n",
    "    \"\"\"This is a function that converts English to numbers. This function has limited ability.\"\"\"\n",
    "    # remove speical characters, like, \", &, etc.\n",
    "    x = x.replace('\"', \"\")\n",
    "    try:  # Convert string to integer\n",
    "        return int(x)\n",
    "    except:\n",
    "        pass\n",
    "    # Convert intergers written in Engligsh in [-10, 10] to numbers\n",
    "    if x == \"negative ten\":\n",
    "        return -10\n",
    "    if x == \"negative nine\":\n",
    "        return -9\n",
    "    if x == \"negative eight\":\n",
    "        return -8\n",
    "    if x == \"negative seven\":\n",
    "        return -7\n",
    "    if x == \"negative six\":\n",
    "        return -6\n",
    "    if x == \"negative five\":\n",
    "        return -5\n",
    "    if x == \"negative four\":\n",
    "        return -4\n",
    "    if x == \"negative three\":\n",
    "        return -3\n",
    "    if x == \"negative two\":\n",
    "        return -2\n",
    "    if x == \"negative one\":\n",
    "        return -1\n",
    "    if x == \"zero\":\n",
    "        return 0\n",
    "    if x == \"one\":\n",
    "        return 1\n",
    "    if x == \"two\":\n",
    "        return 2\n",
    "    if x == \"three\":\n",
    "        return 3\n",
    "    if x == \"four\":\n",
    "        return 4\n",
    "    if x == \"five\":\n",
    "        return 5\n",
    "    if x == \"six\":\n",
    "        return 6\n",
    "    if x == \"seven\":\n",
    "        return 7\n",
    "    if x == \"eight\":\n",
    "        return 8\n",
    "    if x == \"nine\":\n",
    "        return 9\n",
    "    if x == \"ten\":\n",
    "        return 10\n",
    "    return \"FAIL\"\n",
    "\n",
    "\n",
    "def user(x):\n",
    "    if x == \"FAIL\":\n",
    "        return \"The text cannot be converted to a number.\"\n",
    "    if x < 50:\n",
    "        return \"The number needs to be larger.\"\n",
    "    else:\n",
    "        return \"Success.\"\n",
    "\n",
    "\n",
    "def foobar_text(x):\n",
    "    output = convert_english_to_numbers(x)\n",
    "    if output.data == \"FAIL\":  # This is not traced\n",
    "        return output\n",
    "    else:\n",
    "        return foo(bar(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=negative point one, output=FAIL, feedback=The text cannot be converted to a number.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=minus one tenth, output=FAIL, feedback=The text cannot be converted to a number.\n",
      "variable=ten, output=380, feedback=Success.\n",
      "History\n",
      "  0: negative point one\n",
      "  1: minus one tenth\n",
      "  2: ten\n",
      "  3: ten\n"
     ]
    }
   ],
   "source": [
    "GRAPH.clear()\n",
    "x = node(\"negative point one\", trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "history = [x.data]\n",
    "feedback = \"\"\n",
    "while feedback.lower() != \"Success.\".lower():\n",
    "    output = foobar_text(x)\n",
    "    feedback = user(output.data)\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(output, feedback)\n",
    "    print(f\"variable={x.data}, output={output.data}, feedback={feedback}\")  # logging\n",
    "    optimizer.step()\n",
    "    history.append(x.data)  # logging\n",
    "\n",
    "print(\"History\")\n",
    "for i, v in enumerate(history):\n",
    "    print(f\"  {i}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of optimizing functions\n",
    "\n",
    "We can use `trace` to optimize python function directly. This can be achieved by setting `variable=True` when decorating a custom function by `@trace_op`. This would create a `ParameterNode` in the operator, which can be accessed by the `parameter` attribute of the decorated function. It can be used like any other parameters and sent to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output=2, feedback=Try again. The output should be negative, variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return x**2 + 1\n",
      "output=0, feedback=Try again. The output should be negative, variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return 1 - x**2\n",
      "output=0, feedback=Try again. The output should be negative, variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return -(1 - x**2)\n",
      "output=0, feedback=Try again. The output should be negative, variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return -(1 - x**2)\n",
      "output=0, feedback=Try again. The output should be negative, variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return -(1 - x**2)\n",
      "output=0, feedback=Try again. The output should be negative, variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return -(1 - x**2)\n",
      "output=-2, feedback=Success., variables=\n",
      "\n",
      "__code:0 def my_fun(x):\n",
      "    \"\"\"Test function\"\"\"\n",
      "    return -(1 - x**2) - 2\n"
     ]
    }
   ],
   "source": [
    "GRAPH.clear()\n",
    "\n",
    "\n",
    "def user(output):\n",
    "    if output < 0:\n",
    "        return \"Success.\"\n",
    "    else:\n",
    "        return \"Try again. The output should be negative\"\n",
    "\n",
    "\n",
    "# We make this function as a parameter that can be optimized.\n",
    "\n",
    "\n",
    "@trace_op(trainable=True)\n",
    "def my_fun(x):\n",
    "    \"\"\"Test function\"\"\"\n",
    "    return x**2 + 1\n",
    "\n",
    "\n",
    "x = node(-1, trainable=False)\n",
    "optimizer = FunctionOptimizer([my_fun.parameter], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "feedback = \"\"\n",
    "while feedback != \"Success.\":\n",
    "    output = my_fun(x)\n",
    "    feedback = user(output.data)\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(output, feedback)\n",
    "\n",
    "    print(f\"output={output.data}, feedback={feedback}, variables=\\n\")  # logging\n",
    "    for p in optimizer.parameters:\n",
    "        print(p.name, p.data)\n",
    "    optimizer.step(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of hyper-parameter optimization for ML models\n",
    "\n",
    "We can use `trace` to optimize the hyper-parameters of a machine learning model using language feedbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chinganc/miniconda3/envs/autogen/lib/python3.8/site-packages/sklearn/datasets/_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_samples = 10000\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "random_state = check_random_state(0)\n",
    "permutation = random_state.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "X = X.reshape((X.shape[0], -1))\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=train_samples, test_size=20000)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language feedback consists of a text representation of the validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer(classifier, guess, history):\n",
    "    score = classifier.score(X_validation, y_validation) * 100\n",
    "    sparsity = np.mean(classifier.coef_ == 0) * 100\n",
    "    return_feedback = f\"\\nScore is the accuracy of the classifier on the validation set, and should be maximized.\"\n",
    "    return_feedback += f\"\\nSparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.\"\n",
    "    return_feedback += f\"By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\"\n",
    "    return_feedback += f\"\\n\\nMost recent guess: \\nRegularization Parameter: {guess:.4f}, Score: {score:.2f}%, Sparsity: {sparsity:.2f}%\"\n",
    "    if len(history) > 0:\n",
    "        return_feedback += f\"\\n\\nHistory of guesses:\"\n",
    "        for item in history:\n",
    "            return_feedback += (\n",
    "                f\"\\nRegularization Parameter: {item[0]:.4f}, Score: {item[1]:.2f}%, Sparsity: {item[2]:.2f}%\"\n",
    "            )\n",
    "    return return_feedback, score, sparsity\n",
    "\n",
    "\n",
    "@trace_op(trainable=False)\n",
    "def train_classifier(regularization_parameter):\n",
    "    \"\"\"regularization_parameter is a positive number that controls the sparsity of the classifier. Lower values will increase sparsity, and higher values will decrease sparsity.\"\"\"\n",
    "    classifier = LogisticRegression(C=regularization_parameter, penalty=\"l1\", solver=\"saga\", tol=0.1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variable=0.005, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "variable=0.01, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "variable=0.02, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "variable=0.03, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0300, Score: 86.98%, Sparsity: 41.48%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "variable=0.05, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0500, Score: 87.13%, Sparsity: 31.76%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "Regularization Parameter: 0.0300, Score: 86.98%, Sparsity: 41.48%\n",
      "variable=0.06, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0600, Score: 87.25%, Sparsity: 29.85%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "Regularization Parameter: 0.0300, Score: 86.98%, Sparsity: 41.48%\n",
      "Regularization Parameter: 0.0500, Score: 87.13%, Sparsity: 31.76%\n",
      "variable=0.07, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0700, Score: 87.19%, Sparsity: 28.41%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "Regularization Parameter: 0.0300, Score: 86.98%, Sparsity: 41.48%\n",
      "Regularization Parameter: 0.0500, Score: 87.13%, Sparsity: 31.76%\n",
      "Regularization Parameter: 0.0600, Score: 87.25%, Sparsity: 29.85%\n",
      "variable=0.085, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0850, Score: 87.27%, Sparsity: 26.21%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "Regularization Parameter: 0.0300, Score: 86.98%, Sparsity: 41.48%\n",
      "Regularization Parameter: 0.0500, Score: 87.13%, Sparsity: 31.76%\n",
      "Regularization Parameter: 0.0600, Score: 87.25%, Sparsity: 29.85%\n",
      "Regularization Parameter: 0.0700, Score: 87.19%, Sparsity: 28.41%\n",
      "variable=0.09, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0900, Score: 87.27%, Sparsity: 25.83%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "Regularization Parameter: 0.0300, Score: 86.98%, Sparsity: 41.48%\n",
      "Regularization Parameter: 0.0500, Score: 87.13%, Sparsity: 31.76%\n",
      "Regularization Parameter: 0.0600, Score: 87.25%, Sparsity: 29.85%\n",
      "Regularization Parameter: 0.0700, Score: 87.19%, Sparsity: 28.41%\n",
      "Regularization Parameter: 0.0850, Score: 87.27%, Sparsity: 26.21%\n",
      "variable=0.095, feedback=\n",
      "Score is the accuracy of the classifier on the validation set, and should be maximized.\n",
      "Sparsity is the percentage of zero coefficients in the classifier. If the classifier is overfit, a higher sparsity will yield a better score. If the classifier is underfit however, a lower sparsity will yield a better score.By lowering the regularization parameter (must always be positive), the sparsity will increase. By increasing the regularization parameter, the sparsity will decrease.\n",
      "\n",
      "Most recent guess: \n",
      "Regularization Parameter: 0.0950, Score: 87.28%, Sparsity: 25.22%\n",
      "\n",
      "History of guesses:\n",
      "Regularization Parameter: 0.0050, Score: 83.18%, Sparsity: 75.24%\n",
      "Regularization Parameter: 0.0100, Score: 85.45%, Sparsity: 66.77%\n",
      "Regularization Parameter: 0.0200, Score: 86.61%, Sparsity: 46.54%\n",
      "Regularization Parameter: 0.0300, Score: 86.98%, Sparsity: 41.48%\n",
      "Regularization Parameter: 0.0500, Score: 87.13%, Sparsity: 31.76%\n",
      "Regularization Parameter: 0.0600, Score: 87.25%, Sparsity: 29.85%\n",
      "Regularization Parameter: 0.0700, Score: 87.19%, Sparsity: 28.41%\n",
      "Regularization Parameter: 0.0850, Score: 87.27%, Sparsity: 26.21%\n",
      "Regularization Parameter: 0.0900, Score: 87.27%, Sparsity: 25.83%\n",
      "Best regularization parameter: 0.095\n",
      "Best score: 87.285\n"
     ]
    }
   ],
   "source": [
    "x = node(0.005, trainable=True)\n",
    "optimizer = FunctionOptimizer([x], config_list=autogen.config_list_from_json(\"OAI_CONFIG_LIST\"))\n",
    "\n",
    "history = []\n",
    "bestScore = None\n",
    "bestRegularization = None\n",
    "for i in range(10):\n",
    "    classifier = train_classifier(x)\n",
    "    fb, score, sparsity = scorer(classifier.data, x.data, history)\n",
    "    history.append((x.data, score, sparsity))\n",
    "    print(f\"variable={x.data}, feedback={fb}\")  # logging\n",
    "    if bestScore is None or score > bestScore:\n",
    "        bestScore = score\n",
    "        bestRegularization = x.data\n",
    "\n",
    "    optimizer.zero_feedback()\n",
    "    optimizer.backward(classifier, fb)\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Best regularization parameter:\", bestRegularization)\n",
    "print(\"Best score:\", bestScore)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
